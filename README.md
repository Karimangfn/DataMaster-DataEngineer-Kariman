# Data Master - Kariman Gomes

<p align="center">
  <img src="assets/images/Data-Master-Logo.png" style="width:800px; height:250px;">
</p>

O reposit√≥rio "DataMaster-DataEngineer-Kariman" apresenta a solu√ß√£o de Engenharia de Dados criada por [Kariman](https://www.linkedin.com/in/kariman-gomes/) como parte do programa Data Master, uma iniciativa da F1rst Santander. <p>

## üìë Sum√°rio

<details>
  <summary>üìå 1. Vis√£o Geral do Projeto</summary>

  - [In√≠cio R√°pido](#inicio-rapido)

</details>

<details>
  <summary>üèóÔ∏è 2. Arquitetura de Solu√ß√£o</summary>

  - [Vis√£o Geral](#visao-geral)
  - [Diagrama da Arquitetura de Solu√ß√£o](#diagrama-da-arquitetura-de-solucao)
  - [Componentes Principais](#componentes-principais)
  - [Caracter√≠sticas Essenciais do Projeto](#caracteristicas-essenciais-do-projeto)

</details>

<details>
  <summary>‚öôÔ∏è 3. Arquitetura T√©cnica</summary>

  - [Vis√£o Geral T√©cnica](#visao-geral-tecnica)
  - [Descri√ß√£o do Fluxo de Dados](#descricao-do-fluxo-de-dados)
  - [Modelagem e Estrutura do Data Lake](#modelagem-e-estrutura-do-data-lake)
  - [Tecnologias e Servi√ßos Utilizados](#tecnologias-e-servicos-utilizados)
  - [Infraestrutura como C√≥digo](#infraestrutura-como-codigo)
  - [Orquestra√ß√£o de Pipelines](#orquestracao-de-pipelines)
  - [Extra√ß√£o e Ingest√£o de Dados](#extracao-e-ingestao-de-dados)
  - [Armazenamento de Dados](#armazenamento-de-dados)
  - [Processamento e Transforma√ß√£o dos Dados](#processamento-e-transformacao-dos-dados)
  - [Qualidade e Valida√ß√£o de Dados](#qualidade-e-validacao-de-dados)
  - [Mascaramento e Seguran√ßa dos Dados](#mascaramento-e-seguranca-dos-dados)
  - [Observabilidade e Monitoramento](#observabilidade-e-monitoramento)
  - [Escalabilidade e Desempenho](#escalabilidade-e-desempenho)

</details>

<details>
  <summary>üöÄ 4. Guia de Configura√ß√£o e Execu√ß√£o</summary>

  - [Pr√©-requisitos](#pre-requisitos)
  - [Configura√ß√£o da Infraestrutura](#configuracao-da-infraestrutura)
  - [Configura√ß√£o de Credenciais e Acessos](#configuracao-de-credenciais-e-acessos)
  - [Execu√ß√£o dos Pipelines de Ingest√£o](#execucao-dos-pipelines-de-ingestao)
  - [Execu√ß√£o dos Pipelines de Transforma√ß√£o](#execucao-dos-pipelines-de-transformacao)
  - [Execu√ß√£o da Integra√ß√£o com o CRM](#execucao-da-integracao-com-o-crm)

</details>

<details>
  <summary>üí° 5. Melhorias e Considera√ß√µes Finais</summary>

  - [Melhorias Futuras](#melhorias-futuras)
  - [Considera√ß√µes Finais](#consideracoes-finais)

</details>

<details>
  <summary>üìö 6. Refer√™ncias</summary>
</details>

## üìå 1. Vis√£o Geral do Projeto

Este projeto consiste em uma **plataforma de engenharia e experimenta√ß√£o de dados** desenvolvida para demonstrar, em um ambiente controlado e reproduz√≠vel, as principais pr√°ticas de **ingest√£o, processamento, governan√ßa e automa√ß√£o de dados em nuvem**.

A solu√ß√£o foi concebida como uma **infraestrutura completa**, que une conceitos de **engenharia de dados**, **engenharia de software** e **arquitetura em nuvem** para sustentar fluxos de dados **escal√°veis, audit√°veis e seguros**.

### 1.1 Prop√≥sito

A plataforma busca resolver um conjunto mais amplo de desafios enfrentados em ambientes de dados modernos, incluindo:

* Fragmenta√ß√£o de dados em m√∫ltiplas origens e formatos.
* Falta de padroniza√ß√£o e governan√ßa no ciclo de vida dos dados.
* Dificuldade de reprodutibilidade e versionamento de ambientes.
* Baixa automa√ß√£o nos processos de ingest√£o, transforma√ß√£o e deploy.
* Necessidade de garantir seguran√ßa, qualidade e rastreabilidade ponta a ponta.

### 1.2 Abordagem

A solu√ß√£o foi projetada para atender a esses desafios de forma **modular e integrada**, combinando:

* **Arquitetura Medalh√£o (Raw ‚Üí Bronze ‚Üí Silver ‚Üí Gold)** para organizar e evoluir os dados em camadas de qualidade.
* **Infraestrutura como C√≥digo (Terraform)** para provisionar automaticamente os recursos na Azure.
* **Microservi√ßos de Ingest√£o** para integrar m√∫ltiplas fontes de dados de forma independente e escal√°vel.
* **Databricks e Delta Lake** para processamento distribu√≠do, versionamento e confiabilidade transacional.
* **Automa√ß√£o com GitHub Actions** para CI/CD de infraestrutura, c√≥digo e pipelines.
* **Governan√ßa e Seguran√ßa** via RBAC, Service Principals e mascaramento de dados sens√≠veis.
* **Observabilidade e Monitoramento** para acompanhamento de execu√ß√µes e detec√ß√£o de falhas.
* **Boas pr√°ticas de Engenharia de Software**, como Clean Architecture, testes automatizados e valida√ß√µes de qualidade de c√≥digo.

##  üèóÔ∏è 2. Arquitetura de Solu√ß√£o

### 2.1 Vis√£o Geral

A arquitetura foi projetada para sustentar uma plataforma modular de engenharia de dados, capaz de integrar, processar e disponibilizar informa√ß√µes de forma escal√°vel e governada na nuvem.

No centro da solu√ß√£o, encontra-se um pipeline de dados estruturado na arquitetura medalh√£o (Raw ‚Üí Bronze ‚Üí Silver ‚Üí Gold), respons√°vel por orquestrar o fluxo de dados desde a ingest√£o at√© a camada anal√≠tica. Esse pipeline √© suportado por uma infraestrutura automatizada e reprodut√≠vel, provisionada via Terraform, e integrada a componentes de observabilidade, seguran√ßa e automa√ß√£o cont√≠nua.

Os dados s√£o ingeridos de tr√™s origens distintas (banco de dados, API e arquivos) por meio de microservi√ßos independentes, processados no Databricks e armazenados em um Data Lake na Azure.

### 2.2 Diagrama da Arquitetura de Solu√ß√£o

![Figura 1 ‚Äî Arquitetura de Solu√ß√£o](assets/images/Arquitetura-Pipeline-de-Dados.png)

*Figura 1 ‚Äî Arquitetura de solu√ß√£o em alto n√≠vel, mostrando ingest√£o, processamento e armazenamento das camadas do Data Lake.*

### 2.3 Componentes Principais

- **Microservi√ßos de Ingest√£o**: Cada microservi√ßo √© respons√°vel por extrair dados de uma fonte espec√≠fica e grav√°-los na camada raw do Data Lake.
- **Data Lake (Azure Storage Account)**: Armazena os dados em diferentes camadas de processamento (Raw, Bronze, Silver e Gold), seguindo a arquitetura medalh√£o e permitindo rastreabilidade.
- **Databricks**: realiza o processamento e a transforma√ß√£o dos dados. A camada Bronze utiliza o Auto Loader para ingest√£o automatizada, a camada Silver aplica limpeza, padroniza√ß√£o e mascaramento de dados, e a camada Gold gera a tabela pronta para an√°lise.
- **Infraestrutura como C√≥digo (Terraform)**: Provisiona todos os recursos necess√°rios, incluindo ACR, AKS, Storage Account, Databricks e demais componentes da arquitetura.
- **Automa√ß√£o (GitHub Actions)**: Gerencia a cria√ß√£o, o deploy dos microservi√ßos e verifica√ß√£o de qualidade de c√≥digo.

## ‚öôÔ∏è 3. Arquitetura T√©cnica

### 3.1 Descri√ß√£o do Fluxo de Dados

1. **Ingest√£o**: Os microservi√ßos consomem dados das fontes (Banco de Dados, API, Arquivos) e gravam na camada raw do Data Lake. 
2. **Bronze**: Databricks l√™ os dados da raw e cria a tabela Delta, mantendo a integridade das informa√ß√µes.
3. **Silver**: Aplica transforma√ß√µes de limpeza, padroniza√ß√£o e mascaramento de dados sens√≠veis.  
4. **Gold**: Gera tabelas prontas para consumo ou an√°lises.

### 3.2 Tecnologias e Servi√ßos Utilizados

- **Resource Group**: Agrupa todos os recursos provisionados na Azure. 
- **ACR (Azure Container Service)**: Reposit√≥rio para armazenar e versionar as imagens Docker dos microservi√ßos.  
- **AKS (Azure Kubernetes Service)**: Execu√ß√£o dos microservi√ßos de ingest√£o.
- **Storage Account**: Armazenamento do Data Lake com camadas Raw, Bronze, Silver e Gold.
- **Databricks**: Processamento e transforma√ß√£o dos dados.
- **Delta Lake**: Garantia de consist√™ncia, versionamento e ACID nas tabelas.
- **Terraform**: Provisionamento de toda a infraestrutura na Azure.
- **GitHub Actions**: Automa√ß√£o de cria√ß√£o de infraestrutura, deploy de microservi√ßos e execu√ß√£o de jobs.

### 3.3 Infraestrutura como C√≥digo

![Figura 2 ‚Äî Infraestrutura CI/CD](assets/images/Arquitetura-Infrastructure-CI-CD.png)  

*Figura 2 ‚Äî Arquitetura de infraestrutura com Terraform e principais recursos provisionados na Azure.*

Toda a infraestrutura do projeto √© criada com uso de **Terraform**, que tamb√©m salva o estado das cria√ß√µes em artefatos para garantir a consist√™ncia do ambiente e a persist√™ncia do estado entre execu√ß√µes, evitando perdas ou diverg√™ncias durante atualiza√ß√µes.

### Valida√ß√µes (CI)
- **Check Github Token**: Antes da cria√ß√£o dos recursos, √© verificado se o *Personal Access Token* do GitHub est√° criado e configurado com os acessos necess√°rios.

- **Check Azure Role Assignments**: Valida se o *Service Principal* necess√°rio para a cria√ß√£o dos recursos est√° configurado corretamente com as permiss√µes adequadas para atribuir acesso a outro recursos.

- **check Azure Group Permissions**: Valida se o *Service Principal* necess√°rio para a cria√ß√£o dos recursos est√° configurado corretamente com as permiss√µes adequadas para criar grupos na Azure.

### Cria√ß√£o de Recursos (CD)
- **Resource Group**: Respons√°vel por armazenar os recursos.  
- **Azure Container Registry (ACR)**: Armazena as imagens dos microservi√ßos.  
- **Azure Kubernetes Service (AKS)**: Cluster respons√°vel pela execu√ß√£o dos microservi√ßos.
  - Cria√ß√£o de **role assignment** para permitir que o AKS fa√ßa pull das imagens.  
- **Storage Account**: Armazenamento das tabelas de dados.
  - Cria√ß√£o de **role assignment** do Service Principal com permiss√£o de Storage Blob Data Contributor.
- **Storage Containers**: Containers espec√≠ficos para as camadas de dados Raw, Bronze, Silver e Gold.  
- **Databricks**: Cria√ß√£o de recursos e configura√ß√£o do workspace, incluindo:
  - Workspace Databricks
  - Conector de acesso (Access Connector)
  - Cria√ß√£o de External Locations (Bronze, Silver, Gold)
  - Cria√ß√£o de Schema
  - Cria√ß√£o de Job de processamento
  - Cria√ß√£o de Grupos de acesso na workspace
- **Access Groups**: Cria√ß√£o de grupos de acesso para governan√ßa na Azure.

#### Estrutura

```bash
infrastructure/
‚îú‚îÄ‚îÄ main.tf              # Chama os m√≥dulos e define os recursos principais
‚îú‚îÄ‚îÄ variables.tf         # Vari√°veis globais usadas pelos m√≥dulos
‚îú‚îÄ‚îÄ outputs.tf           # Sa√≠das de recursos criados
‚îú‚îÄ‚îÄ providers.tf         # Providers (Azure, Databricks, etc.)
‚îî‚îÄ‚îÄ modules/             # M√≥dulos reutiliz√°veis
    ‚îú‚îÄ‚îÄ access/          # Grupos e permiss√µes (Azure AD / IAM)
    ‚îÇ   ‚îú‚îÄ‚îÄ main.tf
    ‚îÇ   ‚îú‚îÄ‚îÄ variables.tf
    ‚îú‚îÄ‚îÄ acr/             # Azure Container Registry
    ‚îÇ   ‚îú‚îÄ‚îÄ main.tf
    ‚îÇ   ‚îú‚îÄ‚îÄ variables.tf
    ‚îÇ   ‚îî‚îÄ‚îÄ outputs.tf
    ‚îú‚îÄ‚îÄ aks/             # Azure Kubernetes Service
    ‚îÇ   ‚îú‚îÄ‚îÄ main.tf
    ‚îÇ   ‚îú‚îÄ‚îÄ variables.tf
    ‚îÇ   ‚îî‚îÄ‚îÄ outputs.tf
    ‚îú‚îÄ‚îÄ databricks/      # Databricks workspace
    ‚îÇ   ‚îú‚îÄ‚îÄ main.tf
    ‚îÇ   ‚îú‚îÄ‚îÄ variables.tf
    ‚îÇ   ‚îî‚îÄ‚îÄ outputs.tf
    ‚îÇ   ‚îî‚îÄ‚îÄ providers.tf
    ‚îú‚îÄ‚îÄ resource_group/  # Resource Group
    ‚îÇ   ‚îú‚îÄ‚îÄ main.tf
    ‚îÇ   ‚îú‚îÄ‚îÄ variables.tf
    ‚îÇ   ‚îî‚îÄ‚îÄ outputs.tf
    ‚îî‚îÄ‚îÄ storage/         # Storage Account e containers
        ‚îú‚îÄ‚îÄ main.tf
        ‚îú‚îÄ‚îÄ variables.tf
        ‚îî‚îÄ‚îÄ outputs.tf
        ‚îî‚îÄ‚îÄ providers.tf
```

### 3.4 GitHub Actions

O GitHub Actions √© respons√°vel por orquestrar e automatizar todas as etapas do projeto, incluindo:

- **Provisionamento de Infraestrutura**: executa o Terraform para cria√ß√£o e atualiza√ß√£o de recursos na Azure.
- **Orquestra√ß√£o de Pipeline**: coordena a execu√ß√£o de todos os passos, incluindo o disparo dos microservi√ßos de ingest√£o no AKS e dos jobs de transforma√ß√£o no Databricks.
- **Deploy de Microservi√ßos**: build e publica√ß√£o de imagens Docker no ACR, seguido de deploy no AKS.  
- **Transforma√ß√£o de Dados**: integra√ß√£o com Databricks para execu√ß√£o dos pipelines de Bronze, Silver e Gold. 

#### Resumo de Execu√ß√£o (Summary)
Cada execu√ß√£o de workflow gera automaticamente um **summary** dentro do GitHub Actions, contendo:  
- Recursos criados/atualizados pelo Terraform.
- Status de execu√ß√£o das etapas (Infra, Ingest√£o, Transforma√ß√£o).

**Colocar imagem aqui dos summaries depois**

#### Fluxo dos Workflows

**A. Workflows Principais**
1. **Infraestrutura** ‚Üí Provisionamento completo via Terraform  
   1.1 - Deploy Cloud Infrastructure  
2. **Setup de Ingest√£o** ‚Üí Setup de Microservi√ßos no ACR e AKS  
   2.1 - Build and Push to ACR  
   2.2 - AKS Setup and Deploy  
3. **Orquestra√ß√£o** ‚Üí Execu√ß√£o dos Microservi√ßos e Job no Databricks  
   3.1 - Orchestrate Data Pipeline  

**B. Workflows de Qualidade (Pull Requests)**  
1. **Valida√ß√£o de Microservi√ßo** ‚Üí Executa lint, pr√©-commit hooks e testes para os microservi√ßos.
  1.1 - Microservice Quality
2. **Valida√ß√£o de Data Processing** ‚Üí Executa lint, pr√©-commit hooks e testes para o job de Data Processing.
  2.1 - Data Processing Quality

### 3.5 Orquestra√ß√£o de Pipelines

A orquestra√ß√£o dos pipelines √© realizada por um **workflow do GitHub Actions**, que atua como coordenador central da execu√ß√£o de todo o processo de dados. O workflow realiza as seguintes fun√ß√µes:

- Detecta os microservi√ßos que precisam ser executados, lendo arquivos de vers√£o nos reposit√≥rios.
- Dispara os **jobs de ingest√£o no AKS**, validando previamente a exist√™ncia das imagens no ACR.
- Dispara o **job de transforma√ß√£o no Databricks**, garantindo a execu√ß√£o dos pipelines Bronze, Silver e Gold.
- Coleta os resultados das execu√ß√µes, tanto do AKS quanto do Databricks, e gera um resumo consolidado da execu√ß√£o.

- **Colocar Desenho?**

### 3.6 Extra√ß√£o e Ingest√£o de Dados

![Figura 3 ‚Äî Microservi√ßos de Ingest√£o](assets/images/Arquitetura-Microservices-CI-CD.png)  

*Figura 3 ‚Äî Arquitetura de CI/CD dos microservi√ßos de ingest√£o: extra√ß√£o de dados, build, deploy no AKS.*

A ingest√£o dos dados √© realizada por **microservi√ßos** desenvolvidos para cada fonte de dados (Banco de Dados, API e Arquivos). Esses microservi√ßos s√£o empacotados em cont√™ineres e executados em um **Azure Kubernetes Service (AKS)**.

O fluxo segue os seguintes passos:  
1. Cada microservi√ßo se conecta √† sua respectiva fonte.  
2. Os dados s√£o extra√≠dos no formato de origem (JSON ou CSV).  
3. Os microservi√ßos gravam os dados diretamente na **camada Raw** do Data Lake, em pastas espec√≠ficos.  

Caracter√≠sticas principais:
- **Independ√™ncia**: Cada microservi√ßo √© respons√°vel por uma fonte, facilitando manuten√ß√£o e evolu√ß√£o.  
- **Padroniza√ß√£o**: Apesar das diferentes origens, os dados seguem o mesmo schema.  
- **Escalabilidade**: O uso do AKS permite aumentar ou reduzir r√©plicas de ingest√£o conforme a demanda.  
- **Automa√ß√£o**: Pipelines no GitHub Actions garantem que os microservi√ßos sejam validados, constru√≠dos e implantados automaticamente no cluster.

#### Estrutura

```bash
microservice_name/
‚îú‚îÄ‚îÄ helm/               # Helm chart para deploy no AKS
‚îú‚îÄ‚îÄ src/
‚îÇ ‚îú‚îÄ‚îÄ application/      # Regras de neg√≥cio da aplica√ß√£o
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ helpers/        # Fun√ß√µes utilit√°rias
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ services/       # Servi√ßos de orquestra√ß√£o da l√≥gica
‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ validators/     # Valida√ß√µes de entrada e regras espec√≠ficas
‚îÇ ‚îú‚îÄ‚îÄ domain/           # Defini√ß√µes de dom√≠nio
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ exceptions/     # Classes de exce√ß√µes espec√≠ficas
‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ ports/          # Interfaces para comunica√ß√£o entre camadas
‚îÇ ‚îú‚îÄ‚îÄ infrastructure/   # Implementa√ß√µes concretas
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ authentication/ # Autentica√ß√£o e seguran√ßa
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ config/         # Configura√ß√µes de ambiente
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ logging/        # Logs e monitoramento
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ ingestion/      # Conectores e ingest√£o de dados
‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ storage/        # Intera√ß√£o com o Data Lake
‚îÇ ‚îî‚îÄ‚îÄ interfaces/       # Pontos de entrada
‚îÇ ‚îî‚îÄ‚îÄ main.py           # API/CLI principal do microservi√ßo
‚îú‚îÄ‚îÄ tests/              # Testes unit√°rios e de integra√ß√£o
‚îú‚îÄ‚îÄ Dockerfile          # Defini√ß√£o da imagem Docker
‚îú‚îÄ‚îÄ Makefile            # Defini√ß√£o da imagem Docker
‚îú‚îÄ‚îÄ requirements.txt    # Depend√™ncias do servi√ßo
‚îú‚îÄ‚îÄ setup.py            # Defini√ß√£o da imagem Docker
‚îî‚îÄ‚îÄ VERSION             # Usado para versionamento do Microservi√ßo
```

A organiza√ß√£o segue princ√≠pios da Clean Architecture, garantindo separa√ß√£o de responsabilidades e facilidade de manuten√ß√£o:
- **domain** ‚Üí Regras de neg√≥cio puras, interfaces (ports) e exce√ß√µes.
- **application** ‚Üí Casos de uso, servi√ßos, validadores e helpers.
- **infrastructure** ‚Üí Autentica√ß√£o, logging, ingest√£o e persist√™ncia.
- **interfaces** ‚Üí Pontos de entrada do servi√ßo (ex.: main.py).
- **tests** ‚Üí Testes unit√°rios e de integra√ß√£o.
- **helm** ‚Üí Manifests para deploy no AKS.

Arquivos de Configura√ß√£o:
- **.dockerignore** -> Exclui arquivos desnecess√°rios no build Docker.  
- **.flake8** -> Regras de lint para garantir padr√£o de c√≥digo. 
- **.gitignore** -> Define arquivos ignorados no versionamento.
- **.pre-commit-config.yaml** -> Hooks para valida√ß√µes autom√°ticas antes do commit.
- **pytest.ini** -> Configura√ß√µes para execu√ß√£o dos testes com Pytest.

### 3.7 Armazenamento de Dados

O armazenamento dos dados √© realizado em uma **Azure Storage Account**, estruturada segundo a arquitetura medalh√£o.
Cada camada possui um **container dedicado**, garantindo organiza√ß√£o e isolamento entre os est√°gios do pipeline:

- **Raw**: Camada onde os microservi√ßos depositam os dados ingeridos, preservando-os no formato original.  
- **Bronze**: Camada onde o Databricks cria tabelas Delta a partir da Raw, garantindo rastreabilidade e hist√≥rico.  
- **Silver**: Camada que cont√©m dados limpos, padronizados e com mascaramento de informa√ß√µes sens√≠veis.  
- **Gold**: Camada final com dados prontos para consumo em an√°lises e dashboards. 

### 3.8 Processamento e Transforma√ß√£o dos Dados

![Figura 4 ‚Äî Data Processing CI](assets/images/Arquitetura-Data-Processing-CI.png)

*Figura 4 ‚Äî Arquitetura de processamento e transforma√ß√£o de dados em Databricks, seguindo a arquitetura medalh√£o.*

O processamento dos dados √© realizado no **Databricks**, utilizando **Microservi√ßos Python** organizados em tarefas dentro de um **Job**.
Cada job √© dividido em tr√™s etapas principais, alinhadas √† arquitetura medalh√£o:

1. **Bronze**  
   - Leitura dos dados da camada Raw por meio do **Auto Loader** do Databricks.  
   - Cria√ß√£o de tabela **Delta**, preservando os dados ingeridos com hist√≥rico e versionamento.
   - Registro de metadados de rastreabilidade: source_file_name, ingestion_timestamp e raw_ingestion_id (UUID √∫nico por execu√ß√£o).
   - Aplica√ß√£o de controle de acesso garantindo permiss√µes adequadas a grupos de usu√°rios.

  > üí° O processamento incremental √© realizado com o **Auto Loader**, que detecta novos arquivos na camada Raw e processa apenas dados novos.

2. **Silver**  
   - Aplica√ß√£o de regras de **limpeza e padroniza√ß√£o** (ex.: normaliza√ß√£o de formatos, remo√ß√£o de inconsist√™ncias).  
   - Deduplica√ß√£o baseada em customer_id e purchase_date, preservando o registro mais recente.
   - Mascaramento de informa√ß√µes sens√≠veis:
      - cpf ‚Üí SHA-256 ‚Üí coluna cpf_masked
      - credit_card_number ‚Üí SHA-256 ‚Üí coluna credit_card_masked
      - email ‚Üí ofusca√ß√£o parcial mantendo dom√≠nio
   - Enriquecimento dos dados, como a coluna high_value_purchase (flag de compras de alto valor).
   - Preserva√ß√£o das colunas de metadados da Bronze (source_file_name, ingestion_timestamp, raw_ingestion_id).
   - Aplica√ß√£o de controle de acesso garantindo permiss√µes adequadas a grupos de usu√°rios.

3. **Gold**  
   - Agrega√ß√£o dos dados Silver em tabelas anal√≠ticas, prontas para consumo em BI.
   - Cria√ß√£o da coluna derivada purchase_month (formato yyyy-MM) para an√°lises mensais.
   - C√°lculo de m√©tricas agregadas por store_location e purchase_month:
      - total_purchases ‚Üí n√∫mero de compras
      - total_revenue ‚Üí soma do valor total
      - average_purchase_value ‚Üí valor m√©dio de compra
      - high_value_purchases ‚Üí contagem de compras de alto valor
   - Aplica√ß√£o de controle de acesso garantindo permiss√µes adequadas a grupos de usu√°rios.

#### Estrutura

```bash
processing_job/  
‚îú‚îÄ‚îÄ src/  
‚îÇ   ‚îú‚îÄ‚îÄ config/       # Configura√ß√µes do pipeline (par√¢metros, schemas, paths)  
‚îÇ   ‚îú‚îÄ‚îÄ modules/      # M√≥dulos principais de transforma√ß√£o (Bronze, Silver ou Gold)  
‚îÇ   ‚îú‚îÄ‚îÄ utils/        # Fun√ß√µes utilit√°rias reutiliz√°veis  
‚îÇ   ‚îî‚îÄ‚îÄ main.py       # Script principal do job executado no Databricks  
‚îú‚îÄ‚îÄ tests/            # Testes unit√°rios e de integra√ß√£o  
‚îú‚îÄ‚îÄ requirements.txt  # Depend√™ncias do job
```

Arquivos de Configura√ß√£o
- **.flake8** ‚Üí Regras de lint para garantir padr√£o de c√≥digo.
- **.gitignore** ‚Üí Define arquivos ignorados no versionamento.
- **.pre-commit-config.yaml** ‚Üí Hooks para valida√ß√µes autom√°ticas antes do commit.  
- **pytest.ini** ‚Üí Configura√ß√µes para execu√ß√£o dos testes com Pytest.

### 3.9 Qualidade e Valida√ß√£o de Dados

A qualidade dos dados √© garantida a partir da camada **Silver**, onde s√£o aplicadas regras de valida√ß√£o e consist√™ncia antes de disponibilizar as informa√ß√µes para consumo. O processo combina verifica√ß√µes automatizadas e padroniza√ß√µes implementadas nos microservi√ßos do Databricks.

Principais valida√ß√µes aplicadas:  
- **Integridade de schema**: checagem se os dados seguem o schema esperado.  
- **Valores obrigat√≥rios**: verifica√ß√£o de colunas cr√≠ticas que n√£o podem estar nulas (`customer_id`, `cpf`).
- **Convers√£o de tipos**: purchase_date ‚Üí DateType, total_amount ‚Üí DoubleType.
- **Formatos v√°lidos**: confer√™ncia de padr√µes, como CPF v√°lido ou formato correto de e-mail.
- **Deduplica√ß√£o**: remo√ß√£o de registros duplicados mantendo o mais recente por customer_id e purchase_date.
- **Valida√ß√£o de formatos**: emails v√°lidos, datas consistentes, valores num√©ricos coerentes.
- **Valores de dom√≠nio**: confer√™ncia de atributos contra listas pr√©-definidas (ex.: store_location). 

Ferramentas e pr√°ticas:
- Delta Lake ‚Üí versionamento, hist√≥rico, rollback e rastreabilidade.
- Microservi√ßos Python no Databricks ‚Üí implementam todas as valida√ß√µes, limpeza, deduplica√ß√£o e mascaramento.   

**Benef√≠cios principais**:  
- Evita propaga√ß√£o de dados inconsistentes para as camadas anal√≠ticas.  
- Garante confiabilidade e consist√™ncia para relat√≥rios e dashboards.  
- Facilita auditoria e rastreabilidade em caso de erros de ingest√£o ou transforma√ß√£o.

### 3.10 Mascaramento e Seguran√ßa dos Dados

O projeto adota pr√°ticas de **seguran√ßa e privacidade** para proteger informa√ß√µes sens√≠veis dos clientes durante o ciclo de vida dos dados.  
O foco principal est√° no **mascaramento de dados pessoais**, realizado na transi√ß√£o da camada **Bronze ‚Üí Silver**.

#### Mascaramento de Dados
- **CPF**: aplica√ß√£o de hash criptogr√°fico (SHA-256), preservando apenas parte para rastreabilidade.
- **N√∫mero de cart√£o de cr√©dito**: aplica√ß√£o de hash criptogr√°fico (SHA-256), garantindo que os dados originais n√£o sejam expostos. 

Essas transforma√ß√µes s√£o aplicadas atrav√©s dos **Microservi√ßos Python**, garantindo que os dados sens√≠veis n√£o avancem para a camada Gold.

#### Seguran√ßa de Armazenamento e Acesso
- **Azure Storage Account**: Controle de permiss√µes via *role-based access control* (RBAC).  
- **Segrega√ß√£o por camadas**: Cada container (Raw, Bronze, Silver, Gold) possui pol√≠ticas de acesso espec√≠ficas.  
- **GitHub Actions**: Uso de *secrets* para armazenar credenciais de forma segura.  
- **Service Principals**: Autentica√ß√£o entre servi√ßos com permiss√µes m√≠nimas necess√°rias.  

**Benef√≠cios principais**:  
- Prote√ß√£o de informa√ß√µes sens√≠veis em conformidade com boas pr√°ticas de governan√ßa.  
- Redu√ß√£o de riscos em auditorias e conformidade regulat√≥ria (LGPD).  
- Garantia de que dados anal√≠ticos n√£o exponham informa√ß√µes pessoais desnecess√°rias.

### 3.11 Governan√ßa

Nesse projeto, o tema de governan√ßa de dados √© tratado para garantir que cada usu√°rio tenha acesso apenas √†s informa√ß√µes necess√°rias, de acordo com sua fun√ß√£o, promovendo seguran√ßa, rastreabilidade e compliance.

| Persona             | N√≠vel de Acesso                   | Objetivo do Acesso                                              |
|---------------------|-----------------------------------|-----------------------------------------------------------------|
| Engenheiro de Dados | Leitura e Escrita na Raw e Bronze | Implementar e manter pipelines de ingest√£o e transforma√ß√£o.     |
| Cientista de Dados  | Leitura na Silver e Gold          | Realizar an√°lises e modelagem preditiva sobre dados confi√°veis. |
| Analista de Dados   | Leitura na Gold                   | Construir dashboards e relat√≥rios para tomada de decis√£o.       |
| Administrador       | Controle Total                    | Gerenciar usu√°rios, permiss√µes e monitorar seguran√ßa.           |

- Todos os acessos s√£o concedidos seguindo o princ√≠pio do **menor privil√©gio**.  
- O controle √© feito via **RBAC** da Azure Storage Account e Service Principals.   

#### Azure Storage Account (RBAC)

- Engenheiros de dados recebem Storage Blob Data Contributor nos containers Raw e Bronze.
- Cientistas e analistas recebem Storage Blob Data Reader ou Reader parcial nos containers Silver e Gold.

#### Databricks

- Engenheiros de dados configuram e executam os jobs que processam os dados entre os layers (Raw ‚Üí Bronze ‚Üí Silver ‚Üí Gold).
- Cientistas de dados podem acessar resultados processados nos jobs em Silver e Gold.
- Analistas de dados podem executar queries e montar dashboards apenas sobre os dados Gold gerados pelos jobs.
- Administradores t√™m controle total sobre a Storage Account e podem gerenciar jobs, permiss√µes e auditoria no Databricks.

#### Unity Catalog

- O Unity Catalog gerencia tabelas, views, esquemas e fun√ß√µes, garantindo controle de acesso, incluindo permiss√µes em n√≠vel de objeto, linha e coluna.
- **Rastreamento e auditoria**: permite registrar lineage completo, hist√≥rico de altera√ß√µes e acessos aos dados.
- **Democratiza√ß√£o de dados**: usu√°rios podem descobrir e acessar dados de forma segura sem depender de pipelines espec√≠ficos ou da interven√ß√£o de engenheiros de dados.
- **Compartilhamento seguro de dados**: tabelas e views podem ser compartilhadas entre diferentes equipes, unidades de neg√≥cio ou at√© parceiros externos, mantendo o controle sobre quem pode visualizar ou alterar os dados.
- **Padroniza√ß√£o e organiza√ß√£o**: centraliza metadados, schemas e nomenclaturas, garantindo consist√™ncia em todo o Data Lake e em m√∫ltiplas camadas de processamento.

### 3.12 Observabilidade e Monitoramento

O projeto adota logs como principal mecanismo de monitoramento para acompanhar a execu√ß√£o dos pipelines e detectar falhas, garantindo rastreabilidade e confiabilidade nos fluxos de dados.

#### Logs e M√©tricas
- **Microservi√ßos (AKS)**: gera√ß√£o de logs de execu√ß√£o e falhas.
- **Databricks Jobs**: registro autom√°tico de logs de execu√ß√£o, status de tarefas e m√©tricas de processamento.  
- **GitHub Actions**: logs de cada etapa de CI/CD, permitindo auditoria das execu√ß√µes.

### 3.13 Escalabilidade e Desempenho

O projeto foi estruturado para suportar aumento de volume de dados e crescimento no n√∫mero de fontes, mantendo efici√™ncia e confiabilidade no processamento.

#### Escalabilidade
- **Microservi√ßos em AKS**: permitem escalar horizontalmente, aumentando ou reduzindo r√©plicas conforme a demanda de ingest√£o.  
- **Databricks**: os jobs utilizam clusters com autoscale configurado, que realizam escalabilidade horizontal, adicionando ou removendo n√≥s automaticamente conforme o volume de dados processado.
- **Storage Account**: arquitetura baseada em containers independentes para cada camada de dados, possibilitando expans√£o sem necessidade de reestrutura√ß√£o.

#### Desempenho
- **Delta Lake**: garante performance em consultas e manipula√ß√£o de grandes volumes de dados por meio de otimiza√ß√µes internas (indexa√ß√£o, caching e compacta√ß√£o de arquivos).  
- **Auto Loader**: possibilita ingest√£o cont√≠nua e eficiente dos dados da Raw para a Bronze, reduzindo o tempo de lat√™ncia.  
- **Transforma√ß√µes distribu√≠das no Databricks**: uso de processamento paralelo para melhorar a velocidade em opera√ß√µes de limpeza e padroniza√ß√£o.

#### Par√¢metros de Performance e Efici√™ncia do Cluster Databricks 

O cluster Databricks est√° configurado com par√¢metros de otimiza√ß√£o para melhorar a performance de escrita e leitura de dados em tabelas Delta:  

- `spark.databricks.delta.optimizeWrite.enabled = true` ‚Üí reduz o n√∫mero de pequenos arquivos gerados, consolidando-os automaticamente durante a escrita.  
- `spark.databricks.delta.autoCompact.enabled = true` ‚Üí combina arquivos pequenos em arquivos maiores de forma cont√≠nua, melhorando a performance de leitura e queries anal√≠ticas.
- `spark.databricks.delta.schema.autoMerge.enabled = true` ‚Üí Permite que o Delta Lake atualize automaticamente o schema de uma tabela durante opera√ß√µes de merge, append ou overwrite, sem necessidade de recria√ß√£o manual.
- `spark.sql.adaptive.enabled = true` ‚Üí Ativa o Adaptive Query Execution, recurso do Spark que ajusta dinamicamente o plano de execu√ß√£o em tempo de execu√ß√£o (por exemplo, alterando o n√∫mero de parti√ß√µes ou aplicando broadcast joins quando vantajoso), melhorando desempenho de forma autom√°tica.
- `spark.databricks.adaptive.autoOptimizeShuffle = true` ‚Üí Faz parte do AQE e permite que o Databricks reajuste dinamicamente a estrat√©gia de shuffle, otimizando a distribui√ß√£o de dados entre os executores para evitar skew (desequil√≠brio de carga entre parti√ß√µes).
- `spark.sql.adaptive.coalescePartitions.enabled = true` ‚Üí Habilita o ajuste autom√°tico de parti√ß√µes ap√≥s o shuffle, reduzindo o n√∫mero de parti√ß√µes pequenas e otimizando o uso de recursos ao evitar a cria√ß√£o de tarefas muito pequenas (task overhead).
- `spark.databricks.io.cache.enabled = true` ‚Üí Ativa o Databricks I/O Cache, que armazena dados frequentemente acessados em disco local SSD do cluster, reduzindo o tempo de leitura e o custo de reprocessamento em consultas repetitivas.

**Benef√≠cios principais**:  
- Capacidade de lidar com aumento no volume e variedade de dados.  
- Redu√ß√£o da lat√™ncia entre ingest√£o e disponibiliza√ß√£o dos dados anal√≠ticos.  
- Otimiza√ß√£o de custos ao escalar recursos somente quando necess√°rio.

### 3.14 Metodologia de Desenvolvimento

O ciclo de desenvolvimento do projeto segue a estrat√©gia de **GitFlow** para organiza√ß√£o e rastreabilidade do c√≥digo.  

Principais pr√°ticas adotadas:
- **Branch main** ‚Üí sempre est√°vel, representa a vers√£o de produ√ß√£o.  
- **Branch develop** ‚Üí concentra as novas features e integra√ß√µes em andamento.  
- **Feature branches** ‚Üí criadas a partir de `develop` para desenvolvimento de funcionalidades espec√≠ficas.  
- **Release branches** ‚Üí usadas para preparar vers√µes est√°veis antes de ir para produ√ß√£o.  
- **Hotfix branches** ‚Üí permitem corre√ß√µes r√°pidas diretamente na `main`.

##  üöÄ 4. Guia de Configura√ß√£o e Execu√ß√£o

### 4.1 Pr√©-requisitos

Antes de configurar e executar o projeto, √© necess√°rio garantir que o ambiente possua os seguintes pr√©-requisitos:

#### Identidade, Permiss√µes, Credenciais e Acessos
- **Service Principal (SPN)** criado previamente.  
- **Secret da SPN** configurado (*Secret Value*, n√£o o *Secret ID*).  
- A **SPN** precisa ter permiss√µes na assinatura da Azure:  
  - *Contributor*  
  - *User Access Administrator*

#### GitHub Actions
- **Personal Access Token (PAT)** do GitHub criado e salvo nas *Secrets* do reposit√≥rio com permiss√µes adequadas.  

#### Secrets obrigat√≥rias no GitHub
As seguintes *secrets* devem estar configuradas no reposit√≥rio antes da execu√ß√£o de qualquer fluxo de cria√ß√£o:  

- `DB_KEY` ‚Üí Para conex√£o com o Database.  
- `API_KEY` ‚Üí Para conex√£o com a API.  
- `AZURE_CREDENTIALS` ‚Üí Credenciais para conex√£o na Azure, seguindo o seguinte modelo JSON:  
  
  ```json
  {
    "clientId": "", 
    "clientSecret": "", 
    "subscriptionId": "", 
    "tenantId": "" 
  }
  ```

- `GH_PAT_TOKEN` ‚Üí Token criado no GitHub, com as seguintes permiss√µes:

repo ‚Üí Full control of private repositories
repo:status ‚Üí Access commit status
repo_deployment ‚Üí Access deployment status
public_repo ‚Üí Access public repositories
repo:invite ‚Üí Access repository invitations
security_events ‚Üí Read and write security events
workflow ‚Üí Update GitHub Action workflows
write:packages ‚Üí Upload packages to GitHub Package Registry
read:packages ‚Üí Download packages from GitHub Package Registry
admin:org ‚Üí Full control of orgs and teams, read and write org projects
write:org ‚Üí Read and write org and team membership, read and write org projects
read:org ‚Üí Read org and team membership, read org projects
manage_runners:org ‚Üí Manage org runners and runner groups

### **4.2 Cria√ß√£o do Reposit√≥rio a partir do Template**

1. No reposit√≥rio do projeto, acesse **‚ÄúUse this template‚Äù**.
   
   ![Template - 01](assets/images/config-execution/template-01.png)

2. Selecione **‚ÄúCreate a new repository‚Äù**.
   
   ![Template - 02](assets/images/config-execution/template-02.png)

3. Mantenha marcada a op√ß√£o para levar todas as *branches* do reposit√≥rio, defina o nome em **Repository name**, adicione uma descri√ß√£o, configure a visibilidade e clique em **Create repository**.
   
   ![Template - 03](assets/images/config-execution/template-03.png)

4. Aguarde alguns minutos enquanto o reposit√≥rio √© criado.
   
   ![Template - 04](assets/images/config-execution/template-04.png)

---

### **4.3 Configura√ß√£o das Secrets e Vari√°veis de Ambiente**

5. Ap√≥s o reposit√≥rio ser criado, acesse **Settings**.
   
   ![Secrets - 01](assets/images/config-execution/secrets-01.png)

6. No menu √† esquerda, clique em **Secrets and variables**.
   
   ![Secrets - 02](assets/images/config-execution/secrets-02.png)

7. Selecione a op√ß√£o **Actions**.
   
   ![Secrets - 03](assets/images/config-execution/secrets-03.png)

8. Em **Actions secrets and variables**, clique em **New repository secret**.
   
   ![Secrets - 04](assets/images/config-execution/secrets-04.png)

9. A primeira *secret* a ser adicionada ser√° **AZURE_CREDENTIALS**, seguindo o modelo JSON abaixo:
   
   ![Secrets - 05](assets/images/config-execution/secrets-05.png)

10. Em seguida, cadastre o token do GitHub com o nome **GH_PAT_TOKEN**.
   
   ![Secrets - 06](assets/images/config-execution/secrets-06.png)

11. Cadastre a chave para acessar a API, chamada **API_KEY**.
   
   ![Secrets - 07](assets/images/config-execution/secrets-07.png)

12. Cadastre a chave para acessar o banco de dados, chamada **DB_KEY**.
   
   ![Secrets - 08](assets/images/config-execution/secrets-08.png)

13. Ao finalizar, o painel de *secrets* deve se parecer com este:
   
   ![Secrets - 09](assets/images/config-execution/secrets-09.png)

---

### **4.4 Provisionamento da Infraestrutura na Azure**

14. Com tudo configurado, acesse a aba **Actions** no topo do reposit√≥rio.
    
    ![Figura 4 ‚Äî Data Processing CI](assets/images/config-execution/infra-01.png)

15. No menu √† esquerda, selecione o workflow **Deploy Cloud Infrastructure**.
    
    ![Figura 4 ‚Äî Data Processing CI](assets/images/config-execution/infra-02.png)

16. Clique em **Run workflow** e confirme.
    
    ![Figura 4 ‚Äî Data Processing CI](assets/images/config-execution/infra-03.png)

17. Ap√≥s a execu√ß√£o completa, o workflow deve aparecer com todos os *steps* conclu√≠dos.
    
    ![Figura 4 ‚Äî Data Processing CI](assets/images/config-execution/infra-04.png)

18. Verifique na sua conta Azure os **Resource Groups** criados: um para os recursos principais, outro para os recursos base do AKS e outro para os recursos base do Databricks.
    
    ![Figura 4 ‚Äî Data Processing CI](assets/images/config-execution/infra-05.png)

19. O **Resource Group principal** conter√° os recursos criados pelo workflow, incluindo **Databricks, AKS, ACR, Storage Account e Metastore Connector**.
    
    ![Figura 4 ‚Äî Data Processing CI](assets/images/config-execution/infra-06.png)

---

### **4.5 Build e Publica√ß√£o das Imagens Docker**

20. Com a infraestrutura pronta, acesse os workflows e selecione **Build and Push to ACR**.
    
    ![Figura 4 ‚Äî Data Processing CI](assets/images/config-execution/acr-01.png)

21. No menu √† direita, selecione as op√ß√µes de **Run workflow**.
    
    ![Figura 4 ‚Äî Data Processing CI](assets/images/config-execution/acr-02.png)

22. Ap√≥s a execu√ß√£o, verifique no **Azure Container Registry (ACR)** os containers e vers√µes criadas dos microservi√ßos.
    
    ![Figura 4 ‚Äî Data Processing CI](assets/images/config-execution/acr-03.png)

---

### **4.6 Deploy dos Microservi√ßos no AKS**

23. Ap√≥s o workflow do ACR, o workflow do **AKS** √© disparado automaticamente.
    
    ![Figura 4 ‚Äî Data Processing CI](assets/images/config-execution/aks-01.png)

24. Acompanhe no **summary** as vers√µes dos microservi√ßos que est√£o sendo implantadas.
    
    ![Figura 4 ‚Äî Data Processing CI](assets/images/config-execution/aks-02.png)

25. Verifique no **AKS** se os microservi√ßos est√£o em execu√ß√£o.
    
    ![Figura 4 ‚Äî Data Processing CI](assets/images/config-execution/aks-03.png)

---

### **4.7 Execu√ß√£o do Pipeline de Dados**

26. Com tudo instalado, execute o pipeline de ingest√£o e transforma√ß√£o de dados, selecionando o workflow **Orchestrate Data Pipeline**.
    
    ![Figura 4 ‚Äî Data Processing CI](assets/images/config-execution/pipe-01.png)

27. Clique em **Run workflow**.
    
    ![Figura 4 ‚Äî Data Processing CI](assets/images/config-execution/pipe-02.png)

28. Ap√≥s a execu√ß√£o, verifique no **AKS** se os jobs foram conclu√≠dos com sucesso.
    
    ![Figura 4 ‚Äî Data Processing CI](assets/images/config-execution/pipe-03.png)

29. Por fim, confirme no **Databricks** a execu√ß√£o do job de transforma√ß√£o.
    
    ![Figura 4 ‚Äî Data Processing CI](assets/images/config-execution/pipe-04.png)


##  üí° 5. Melhorias e Considera√ß√µes Finais

###  5.1 Melhorias Futuras

- **Seguran√ßa de Rede (VNet e Private Endpoints):** inclus√£o de Virtual Networks e Private Endpoints para isolar os recursos da Azure e garantir maior seguran√ßa no tr√°fego de dados. 
- **Fluxo de Ambientes (Dev / Pre / Prod):** implementa√ß√£o de m√∫ltiplos ambientes com pipelines de deploy independentes, possibilitando testes e valida√ß√µes antes de subir em produ√ß√£o.    
- **Observabilidade e Alertas:** centralizar m√©tricas, logs e alertas em ferramentas como **Azure Monitor** ou **Grafana**, possibilitando detec√ß√£o proativa de falhas. 
- **Infraestrutura como C√≥digo Din√¢mica:** permitir que a pr√≥pria GitHub Action altere a infraestrutura via Terraform ou equivalente, utilizando um backend para salvar estado.
- **Sincroniza√ß√£o de Grupos AD e Databricks:** automatizar a sincroniza√ß√£o de grupos do Azure AD com os grupos de Databricks, reduzindo etapas manuais de configura√ß√£o de permiss√µes e acesso.
- **Contratos de Dados:** ado√ß√£o de contratos de dados para permitir ingest√£o de m√∫ltiplos tipos de schemas.
- **Pipelines Version√°veis e End-to-End:** possibilitar desenvolvimento de m√∫ltiplos pipelines version√°veis, desacoplados do Terraform fixo, integrados com os contratos de dados, permitindo criar novos fluxos de ingest√£o.
- **Expurgo da Camada Raw** : remover dados da camada Raw ap√≥s a ingest√£o e valida√ß√£o, reduzindo custos de armazenamento.
- **Suporte a Streaming**: estender a arquitetura atual para incluir pipelines de ingest√£o e processamento streaming, tornando a plataforma h√≠brida (batch + streaming) e preparada para casos de uso em tempo real.
- **Abstra√ß√£o Multicloud**: reestruturar a camada de infraestrutura e os microservi√ßos para desacoplar depend√™ncias espec√≠ficas da Azure, viabilizando execu√ß√£o em m√∫ltiplos provedores de nuvem (AWS, GCP, Azure), com conectores e provisionamento agn√≥sticos.
- **Evolu√ß√£o do fluxo de monitoramento**: aprimorar o monitoramento atual, baseado em logs de microservi√ßos, Databricks Jobs e GitHub Actions, centralizando m√©tricas, dashboards e alertas em ferramentas como Azure Monitor ou Grafana.

###  5.2 Considera√ß√µes Finais

O projeto demonstrou como √© poss√≠vel integrar dados de m√∫ltiplas fontes e formatos, organizando-os em um pipeline escal√°vel e confi√°vel na Azure.  
A solu√ß√£o aplicou boas pr√°ticas de engenharia de dados (arquitetura medalh√£o, uso de Delta Lake, pipelines automatizados) em conjunto com boas pr√°ticas de engenharia de software (microservi√ßos, CI/CD, IaC).  

Embora ainda existam pontos de evolu√ß√£o, a arquitetura atual j√° fornece uma base s√≥lida para ingest√£o, transforma√ß√£o e disponibiliza√ß√£o de dados prontos para an√°lise, podendo ser expandida gradualmente conforme as necessidades de neg√≥cio e requisitos de governan√ßa cres√ßam.

##  üìö 6. Refer√™ncias
