# Data Master - Kariman Gomes

<p align="center">
  <img src="assets/images/Data-Master-Logo.png" style="width:800px; height:250px;">
</p>

O reposit√≥rio "DataMaster-DataEngineer-Kariman" apresenta a solu√ß√£o de Engenharia de Dados criada por [Kariman](https://www.linkedin.com/in/kariman-gomes/) como parte do programa Data Master, uma iniciativa da F1rst Santander. <p>

## üìë Sum√°rio

<details>
  <summary>üìå 1. Objetivo do Projeto</summary>

  - [In√≠cio R√°pido](#inicio-rapido)

</details>

<details>
  <summary>üèóÔ∏è 2. Arquitetura de Solu√ß√£o</summary>

  - [Vis√£o Geral](#visao-geral)
  - [Diagrama da Arquitetura de Solu√ß√£o](#diagrama-da-arquitetura-de-solucao)
  - [Componentes Principais](#componentes-principais)
  - [Caracter√≠sticas Essenciais do Projeto](#caracteristicas-essenciais-do-projeto)

</details>

<details>
  <summary>‚öôÔ∏è 3. Arquitetura T√©cnica</summary>

  - [Vis√£o Geral T√©cnica](#visao-geral-tecnica)
  - [Descri√ß√£o do Fluxo de Dados](#descricao-do-fluxo-de-dados)
  - [Modelagem e Estrutura do Data Lake](#modelagem-e-estrutura-do-data-lake)
  - [Tecnologias e Servi√ßos Utilizados](#tecnologias-e-servicos-utilizados)
  - [Infraestrutura como C√≥digo](#infraestrutura-como-codigo)
  - [Orquestra√ß√£o de Pipelines](#orquestracao-de-pipelines)
  - [Extra√ß√£o e Ingest√£o de Dados](#extracao-e-ingestao-de-dados)
  - [Armazenamento de Dados](#armazenamento-de-dados)
  - [Processamento e Transforma√ß√£o dos Dados](#processamento-e-transformacao-dos-dados)
  - [Qualidade e Valida√ß√£o de Dados](#qualidade-e-validacao-de-dados)
  - [Mascaramento e Seguran√ßa dos Dados](#mascaramento-e-seguranca-dos-dados)
  - [Observabilidade e Monitoramento](#observabilidade-e-monitoramento)
  - [Escalabilidade e Desempenho](#escalabilidade-e-desempenho)

</details>

<details>
  <summary>üöÄ 4. Guia de Configura√ß√£o e Execu√ß√£o</summary>

  - [Pr√©-requisitos](#pre-requisitos)
  - [Configura√ß√£o da Infraestrutura](#configuracao-da-infraestrutura)
  - [Configura√ß√£o de Credenciais e Acessos](#configuracao-de-credenciais-e-acessos)
  - [Execu√ß√£o dos Pipelines de Ingest√£o](#execucao-dos-pipelines-de-ingestao)
  - [Execu√ß√£o dos Pipelines de Transforma√ß√£o](#execucao-dos-pipelines-de-transformacao)
  - [Execu√ß√£o da Integra√ß√£o com o CRM](#execucao-da-integracao-com-o-crm)

</details>

<details>
  <summary>üí° 5. Melhorias e Considera√ß√µes Finais</summary>

  - [Melhorias Futuras](#melhorias-futuras)
  - [Considera√ß√µes Finais](#consideracoes-finais)

</details>

<details>
  <summary>üí∞ 6. Custos do Projeto</summary>
</details>

<details>
  <summary>üìö 7. Refer√™ncias</summary>
</details>

##  üìå 1. Objetivo do Projeto

Este projeto busca integrar dados de clientes espalhados em diferentes fontes e formatos, que dificultam a integra√ß√£o, padroniza√ß√£o e an√°lise unificada em um √∫nico reposit√≥rio estruturado, permitindo que os dados sejam processados, transformados e analisados de maneira consistente e confi√°vel, seguindo boas pr√°ticas de engenharia de dados e de software.

### 1.1 Problema

Atualmente, os dados de clientes encontram-se distribu√≠dos em diversas fontes, como bancos de dados, APIs e arquivos CSV. Essa fragmenta√ß√£o gera dificuldades como:  
- Redund√¢ncia e inconsist√™ncia entre registros.  
- Dificuldade de padroniza√ß√£o e integra√ß√£o dos dados.  
- Falta de rastreabilidade e governan√ßa.  
- Barreiras para an√°lises unificadas e confi√°veis.

### 1.2 Solu√ß√£o

A solu√ß√£o proposta √© construir um pipeline de dados baseado na arquitetura medalh√£o (Raw ‚Üí Bronze ‚Üí Silver ‚Üí Gold), que:  
- Ingesta dados de m√∫ltiplas fontes por meio de microservi√ßos em AKS.  
- Centraliza e organiza as informa√ß√µes em um Data Lake na Azure.  
- Processa e transforma os dados utilizando Databricks e Delta Lake.  
- Aplica limpeza, padroniza√ß√£o e mascaramento de dados sens√≠veis.  
- Disponibiliza camadas de dados confi√°veis e prontas para consumo anal√≠tico.

##  üèóÔ∏è 2. Arquitetura de Solu√ß√£o

### 2.1 Vis√£o Geral

A arquitetura do case foi projetada para integrar dados de diferentes fontes e garantir que eles estejam dispon√≠veis de forma consistente e confi√°vel para an√°lise. Os dados s√£o ingeridos de tr√™s origens distintas (banco de dados, API e arquivos CSV) por meio de microservi√ßos, processados e transformados em camadas de dados estruturadas (Bronze ‚Üí Silver ‚Üí Gold) e armazenados em um Data Lake na Azure.

O fluxo de dados segue a arquitetura medalh√£o, garantindo que cada camada tenha dados com n√≠veis crescentes de qualidade e consist√™ncia, desde a ingest√£o bruta at√© a camada pronta para an√°lise.

### 2.2 Diagrama da Arquitetura de Solu√ß√£o

![Figura 1 ‚Äî Arquitetura de Solu√ß√£o](assets/images/Arquitetura-Pipeline-de-Dados.png)

*Figura 1 ‚Äî Arquitetura de solu√ß√£o em alto n√≠vel, mostrando ingest√£o, processamento e armazenamento das camadas do Data Lake.*

### 2.3 Componentes Principais

- **Microservi√ßos de Ingest√£o (AKS)**: cada microservi√ßo √© respons√°vel por extrair dados de uma fonte espec√≠fica e grav√°-los na camada raw do Data Lake. Eles podem ser detalhados posteriormente quanto √† linguagem, endpoints e bibliotecas utilizadas.  
- **Data Lake (Azure Storage Account)**: armazena os dados em diferentes camadas de processamento (Raw, Bronze, Silver e Gold), seguindo a arquitetura medalh√£o e permitindo rastreabilidade e governan√ßa.  
- **Databricks**: realiza o processamento e a transforma√ß√£o dos dados. A camada Bronze utiliza o Auto Loader para ingest√£o automatizada, a camada Silver aplica limpeza, padroniza√ß√£o e mascaramento de dados, e a camada Gold gera tabelas prontas para an√°lise.  
- **Infraestrutura como C√≥digo (Terraform)**: provisiona todos os recursos necess√°rios, incluindo AKS, Storage Account, Databricks e demais componentes da arquitetura, garantindo consist√™ncia e reprodutibilidade.  
- **Automa√ß√£o (GitHub Actions)**: gerencia a cria√ß√£o e atualiza√ß√£o da infraestrutura, o deploy dos microservi√ßos e a execu√ß√£o dos jobs no Databricks, garantindo integra√ß√£o cont√≠nua e pipelines reproduz√≠veis.

### 2.4 Caracter√≠sticas Essenciais do Projeto

## ‚öôÔ∏è 3. Arquitetura T√©cnica

### 3.1 Vis√£o Geral T√©cnica

O projeto utiliza uma arquitetura em nuvem na Azure, combinando pr√°ticas de engenharia de dados e engenharia de software. A infraestrutura √© provisionada via Terraform e composta por microservi√ßos para ingest√£o, Data Lake para armazenamento estruturado, Databricks para processamento e pipelines automatizados com GitHub Actions. O objetivo t√©cnico √© permitir a ingest√£o, transforma√ß√£o e disponibiliza√ß√£o de dados confi√°veis e padronizados em um ambiente reproduz√≠vel e escal√°vel.

### 3.2 Descri√ß√£o do Fluxo de Dados

1. **Ingest√£o**: os microservi√ßos consomem dados das fontes (banco, API, CSV) e gravam na camada raw do Data Lake.  
2. **Bronze**: Databricks l√™ os dados da raw e cria tabelas Delta, registrando metadados importantes e mantendo a integridade das informa√ß√µes.  
3. **Silver**: aplica transforma√ß√µes de limpeza, padroniza√ß√£o e mascaramento de dados sens√≠veis.  
4. **Gold**: gera tabelas anal√≠ticas prontas para consumo por ferramentas de BI ou an√°lises avan√ßadas.  

### 3.3 Tecnologias e Servi√ßos Utilizados

Azure Storage Account: Armazenamento do Data Lake com camadas Raw, Bronze, Silver e Gold.
AKS (Azure Kubernetes Service): Execu√ß√£o dos microservi√ßos de ingest√£o.
Databricks: Processamento e transforma√ß√£o dos dados.
Terraform: Provisionamento de toda a infraestrutura na Azure.
GitHub Actions: Automa√ß√£o de cria√ß√£o de infraestrutura, deploy de microservi√ßos e execu√ß√£o de jobs.
Delta Lake: Garantia de consist√™ncia, versionamento e ACID nas tabelas.

### 3.4 Infraestrutura como C√≥digo

![Figura 2 ‚Äî Infraestrutura CI/CD](assets/images/Arquitetura-Infrastructure-CI-CD.png)  

*Figura 2 ‚Äî Arquitetura de infraestrutura com Terraform e principais recursos provisionados na Azure.*

Toda a infraestrutura do projeto √© criada com uso de **Terraform**, que tamb√©m salva o estado das cria√ß√µes para permitir atualiza√ß√µes ou exclus√£o da infraestrutura.

### Valida√ß√µes
- **Check GH PAT**: antes da cria√ß√£o dos recursos, √© verificado se o *Personal Access Token* do GitHub est√° criado e configurado com os acessos necess√°rios.  
- **Check Azure Role Assignments**: valida se o *Service Principal* necess√°rio para a cria√ß√£o dos recursos est√° configurado corretamente com as permiss√µes adequadas.  

### Recursos
- **Resource Group**: respons√°vel por armazenar os recursos.  
- **Azure Container Registry (ACR)**: armazena as imagens dos microservi√ßos.  
- **Azure Kubernetes Service (AKS)**: cluster respons√°vel pela execu√ß√£o dos microservi√ßos.  
- **Storage Account**: armazenamento das tabelas de dados.  
- **Storage Containers**: containers espec√≠ficos para as camadas de dados Raw, Bronze, Silver e Gold.  
- **Azure Databricks**: utilizado para o processamento de dados, desde a camada Raw at√© a camada Gold.  

### 3.5 Orquestra√ß√£o de Pipelines

Orquestra√ß√£o ainda em desenvolvimento

### 3.6 Extra√ß√£o e Ingest√£o de Dados

![Figura 3 ‚Äî Microservi√ßos de Ingest√£o](assets/images/Arquitetura-Microservices-CI-CD.png)  

*Figura 3 ‚Äî Arquitetura de CI/CD dos microservi√ßos de ingest√£o: extra√ß√£o de dados, build, deploy no AKS.*

A ingest√£o dos dados √© realizada por **microservi√ßos** desenvolvidos para cada fonte de dados (banco de dados, API e arquivos CSV). Esses microservi√ßos s√£o empacotados em cont√™ineres e executados em um **Azure Kubernetes Service (AKS)**, garantindo escalabilidade e isolamento de processos.  

O fluxo segue os seguintes passos:  
1. Cada microservi√ßo se conecta √† sua respectiva fonte.  
2. Os dados s√£o extra√≠dos no formato de origem (JSON ou CSV).  
3. Os microservi√ßos gravam os dados diretamente na **camada Raw** do Data Lake, em containers espec√≠ficos.  

Caracter√≠sticas principais:  
- **Independ√™ncia**: cada microservi√ßo √© respons√°vel por uma fonte, facilitando manuten√ß√£o e evolu√ß√£o.  
- **Padroniza√ß√£o**: apesar das diferentes origens, os dados seguem o mesmo schema.  
- **Escalabilidade**: o uso do AKS permite aumentar ou reduzir r√©plicas de ingest√£o conforme a demanda.  
- **Automa√ß√£o**: pipelines no GitHub Actions garantem que os microservi√ßos sejam validados, constru√≠dos e implantados automaticamente no cluster.  

### 3.7 Armazenamento de Dados

O armazenamento dos dados √© realizado em uma **Storage Account da Azure**, estruturada segundo a arquitetura medalh√£o.  
Cada camada possui um **container dedicado**, garantindo organiza√ß√£o e isolamento entre os est√°gios do pipeline:

- **Raw**: camada onde os microservi√ßos depositam os dados ingeridos, preservando-os no formato original.  
- **Bronze**: camada onde o Databricks cria tabelas Delta a partir da Raw, garantindo rastreabilidade e hist√≥rico.  
- **Silver**: camada que cont√©m dados limpos, padronizados e com mascaramento de informa√ß√µes sens√≠veis.  
- **Gold**: camada final com dados prontos para consumo em an√°lises e dashboards.  

### 3.8 Processamento e Transforma√ß√£o dos Dados

![Figura 4 ‚Äî Data Processing CI](assets/images/Arquitetura-Data-Processing-CI.png)

*Figura 4 ‚Äî Arquitetura de processamento e transforma√ß√£o de dados em Databricks, seguindo a arquitetura medalh√£o.*

O processamento dos dados √© realizado no **Azure Databricks**, utilizando **notebooks Python** organizados em tarefas dentro de um **Databricks Job**.  
Cada job √© dividido em tr√™s etapas principais, alinhadas √† arquitetura medalh√£o:

1. **Bronze**  
   - Leitura dos dados da camada Raw por meio do **Auto Loader** do Databricks.  
   - Cria√ß√£o de tabelas **Delta Lake**, preservando os dados ingeridos com hist√≥rico e versionamento.  
   - Registro de metadados iniciais para rastreabilidade.  

2. **Silver**  
   - Aplica√ß√£o de regras de **limpeza e padroniza√ß√£o** (ex.: normaliza√ß√£o de formatos, remo√ß√£o de inconsist√™ncias).  
   - **Mascaramento de informa√ß√µes sens√≠veis**, como CPF e n√∫mero de cart√£o de cr√©dito.  
   - Enriquecimento dos dados quando necess√°rio.  

3. **Gold**  
   - Estrutura√ß√£o dos dados em modelos anal√≠ticos.  
   - Prepara√ß√£o das tabelas para consumo em ferramentas de BI e relat√≥rios.  
   - Disponibiliza√ß√£o de dados consistentes e confi√°veis para an√°lise.

### 3.9 Qualidade e Valida√ß√£o de Dados

A qualidade dos dados √© garantida a partir da camada **Silver**, onde s√£o aplicadas regras de valida√ß√£o e consist√™ncia antes de disponibilizar as informa√ß√µes para consumo.  
O processo combina verifica√ß√µes automatizadas e padroniza√ß√µes implementadas nos notebooks do Databricks.

Principais valida√ß√µes aplicadas:  
- **Integridade de schema**: checagem se os dados seguem o schema esperado.  
- **Valores obrigat√≥rios**: verifica√ß√£o de colunas cr√≠ticas que n√£o podem estar nulas (ex.: `customer_id`, `cpf`).  
- **Formatos v√°lidos**: confer√™ncia de padr√µes, como CPF v√°lido ou formato correto de e-mail.  
- **Valores de dom√≠nio**: valida√ß√£o de atributos contra listas pr√©-definidas (ex.: localiza√ß√£o de lojas).  

Ferramentas e pr√°ticas:  
- **Delta Lake** para versionamento e rollback em caso de ingest√£o incorreta.  
- **Notebooks Python no Databricks** com fun√ß√µes de valida√ß√£o.  
- Possibilidade futura de integra√ß√£o com frameworks como **Great Expectations** para valida√ß√µes mais complexas e documenta√ß√£o de regras de qualidade.  

**Benef√≠cios principais**:  
- Evita propaga√ß√£o de dados inconsistentes para as camadas anal√≠ticas.  
- Garante confiabilidade e consist√™ncia para relat√≥rios e dashboards.  
- Facilita auditoria e rastreabilidade em caso de erros de ingest√£o ou transforma√ß√£o.  

### 3.10 Mascaramento e Seguran√ßa dos Dados

O projeto adota pr√°ticas de **seguran√ßa e privacidade** para proteger informa√ß√µes sens√≠veis dos clientes durante o ciclo de vida dos dados.  
O foco principal est√° no **mascaramento de dados pessoais**, realizado na transi√ß√£o da camada **Bronze ‚Üí Silver**.

#### Mascaramento de Dados
- **CPF**: substitui√ß√£o parcial dos d√≠gitos, preservando apenas os √∫ltimos 3 para rastreabilidade.  
- **N√∫mero de cart√£o de cr√©dito**: oculta√ß√£o de todos os d√≠gitos, exceto os 4 √∫ltimos.  
- **E-mail**: ofusca√ß√£o parcial do endere√ßo, mantendo o dom√≠nio vis√≠vel.  

Essas transforma√ß√µes s√£o aplicadas via **notebooks Python no Databricks**, garantindo que os dados sens√≠veis n√£o avancem para a camada Gold.

#### Seguran√ßa de Armazenamento e Acesso
- **Azure Storage Account**: controle de permiss√µes via *role-based access control* (RBAC).  
- **Segrega√ß√£o por camadas**: cada container (Raw, Bronze, Silver, Gold) possui pol√≠ticas de acesso espec√≠ficas.  
- **GitHub Actions**: uso de *secrets* para armazenar credenciais de forma segura.  
- **Service Principals**: autentica√ß√£o entre servi√ßos com permiss√µes m√≠nimas necess√°rias.  

**Benef√≠cios principais**:  
- Prote√ß√£o de informa√ß√µes sens√≠veis em conformidade com boas pr√°ticas de governan√ßa.  
- Redu√ß√£o de riscos em auditorias e conformidade regulat√≥ria (LGPD).  
- Garantia de que dados anal√≠ticos n√£o exponham informa√ß√µes pessoais desnecess√°rias.  

### 3.11 Observabilidade e Monitoramento

O projeto prev√™ mecanismos de **observabilidade e monitoramento** para acompanhar a execu√ß√£o dos pipelines, identificar falhas rapidamente e garantir confiabilidade no fluxo de dados.

#### Logs e M√©tricas
- **Microservi√ßos (AKS)**: gera√ß√£o de logs de execu√ß√£o e falhas, que podem ser integrados ao **Azure Monitor** ou **Application Insights**.  
- **Databricks Jobs**: registro autom√°tico de logs de execu√ß√£o, status de tarefas e m√©tricas de processamento.  
- **GitHub Actions**: logs detalhados de cada etapa de CI/CD, permitindo auditoria das execu√ß√µes.  

#### Alertas
- Configura√ß√£o de alertas no **Azure Monitor** para falhas em microservi√ßos ou indisponibilidade de recursos.  
- Alertas no **Databricks** para jobs que falharem ou ultrapassarem o tempo limite de execu√ß√£o.  

#### Observabilidade de Dados
- M√©tricas de volume de dados ingeridos em cada fonte.  
- Taxa de erros de ingest√£o ou registros inv√°lidos.  
- Possibilidade futura de integra√ß√£o com ferramentas de **Data Lineage** para rastreabilidade ponta a ponta.  

**Benef√≠cios principais**:  
- Redu√ß√£o do tempo de detec√ß√£o e resposta a falhas.  
- Garantia de confiabilidade e disponibilidade dos pipelines.  
- Maior transpar√™ncia sobre a sa√∫de do ecossistema de dados.  

### 3.12 Escalabilidade e Desempenho

O projeto foi estruturado para suportar aumento de volume de dados e crescimento no n√∫mero de fontes, mantendo efici√™ncia e confiabilidade no processamento.

#### Escalabilidade
- **Microservi√ßos em AKS**: permitem escalar horizontalmente, aumentando ou reduzindo r√©plicas conforme a demanda de ingest√£o.  
- **Databricks**: suporte a clusters escal√°veis sob demanda, otimizando custo e performance no processamento das camadas Bronze, Silver e Gold.  
- **Storage Account**: arquitetura baseada em containers independentes para cada camada de dados, possibilitando expans√£o sem necessidade de reestrutura√ß√£o.  

#### Desempenho
- **Delta Lake**: garante performance em consultas e manipula√ß√£o de grandes volumes de dados por meio de otimiza√ß√µes internas (indexa√ß√£o, caching e compacta√ß√£o de arquivos).  
- **Auto Loader**: possibilita ingest√£o cont√≠nua e eficiente dos dados da Raw para a Bronze, reduzindo o tempo de lat√™ncia.  
- **Transforma√ß√µes distribu√≠das no Databricks**: uso de processamento paralelo para melhorar a velocidade em opera√ß√µes de limpeza e padroniza√ß√£o.  

**Benef√≠cios principais**:  
- Capacidade de lidar com aumento no volume e variedade de dados.  
- Redu√ß√£o da lat√™ncia entre ingest√£o e disponibiliza√ß√£o dos dados anal√≠ticos.  
- Otimiza√ß√£o de custos ao escalar recursos somente quando necess√°rio.

##  üöÄ 4. Guia de Configura√ß√£o e Execu√ß√£o

### 4.1 Pr√©-requisitos

Antes de configurar e executar o projeto, √© necess√°rio garantir que o ambiente possua os seguintes pr√©-requisitos:

#### Identidade e Permiss√µes
- **Service Principal (SPN)** criado previamente.  
- **Secret da SPN** configurado (Secret Value, n√£o o Secret ID).  
- A **SPN** precisa ter permiss√µes na assinatura da Azure:  
  - *Contributor*  
  - *User Access Administrator*  
- A **SPN** deve ter tamb√©m a role **Storage Blob Data Contributor** associada.  

#### Configura√ß√£o do AKS e ACR
- O nome do **ACR** deve ser informado **sem o sufixo** `.azurecr.io`.  
- O **AKS** √© privado, logo os comandos devem ser executados via `az aks command invoke`.  
- O n√≥ do AKS deve ser baseado em arquitetura **AMD** (ARM n√£o √© suportado).  

#### GitHub Actions
- **Workflow permissions** devem estar configuradas como **Read and Write**.  
- **Personal Access Token (PAT)** do GitHub criado e salvo nas *Secrets* do reposit√≥rio com permiss√µes adequadas.  

#### Configura√ß√µes de Rede
- O recurso **Network Watcher NG** precisa estar desabilitado na regi√£o da implanta√ß√£o.  

#### Secrets obrigat√≥rias no GitHub
As seguintes *secrets* devem estar configuradas no reposit√≥rio:  
- `DB_KEY`  
- `API_KEY`  
- `ACR_NAME`  
- `AKS_NAME`  
- `STORAGE_ACCOUNT`  
- `AZURE_CREDENTIALS`  
- `RESOURCE_GROUP`  
- `GH_PAT_TOKEN`  

### 4.2 Configura√ß√£o da Infraestrutura

A cria√ß√£o e configura√ß√£o da infraestrutura do projeto √© realizada de forma automatizada utilizando **Terraform** em conjunto com **GitHub Actions**.  
Esse processo garante reprodutibilidade, versionamento e consist√™ncia em todas as implanta√ß√µes.

#### Passos principais

1. **Clonar o reposit√≥rio**
   ```bash
   git clone https://github.com/<usuario>/<repo>.git
   cd <repo>
Configurar as secrets no GitHub

Definir todas as secrets obrigat√≥rias listadas na se√ß√£o Pr√©-requisitos.

Garantir que o workflow permissions esteja configurado como Read and Write.

Executar o workflow de cria√ß√£o de infraestrutura

O workflow do GitHub Actions respons√°vel pela cria√ß√£o da infraestrutura deve ser acionado manualmente (workflow_dispatch) ou via push para o branch principal.

Esse pipeline realiza:

Valida√ß√£o de credenciais e permiss√µes.

Cria√ß√£o/atualiza√ß√£o do Resource Group.

Provisionamento do Storage Account com containers (Raw, Bronze, Silver, Gold).

Cria√ß√£o do Azure Container Registry (ACR).

Deploy do Azure Kubernetes Service (AKS).

Configura√ß√£o do Azure Databricks.

Validar a implanta√ß√£o

Confirmar que todos os recursos foram criados no Resource Group especificado.

Testar conectividade do AKS utilizando:

bash
Copiar c√≥digo
az aks command invoke --name <aks-name> --resource-group <rg-name> --command "kubectl get pods -A"
Verificar se os containers do Storage est√£o organizados conforme a arquitetura medalh√£o.

Observa√ß√µes
O Terraform utiliza Remote State armazenado nos artifacts do GitHub Actions, permitindo atualiza√ß√µes e destrui√ß√£o da infraestrutura de forma segura.

Existe um workflow espec√≠fico para exclus√£o completa da infraestrutura, que deve ser executado com cautela.

### 4.3 Configura√ß√£o de Credenciais e Acessos

Para garantir que a infraestrutura e os pipelines de dados sejam provisionados e executados corretamente, √© necess√°rio configurar credenciais e permiss√µes adequadas no **Azure** e no **GitHub**.

#### 1. Service Principal (SPN)
O projeto utiliza uma **Service Principal (SPN)** para autentica√ß√£o e cria√ß√£o de recursos no Azure.

- Cria√ß√£o da SPN:
  ```bash
  az ad sp create-for-rbac \
    --name spn-datamaster \
    --role Contributor \
    --scopes /subscriptions/<SUBSCRIPTION_ID>
Permiss√µes necess√°rias:

Contributor ‚Üí cria√ß√£o e gerenciamento dos recursos.

User Access Administrator ‚Üí atribui√ß√£o de permiss√µes a outros recursos/usu√°rios.

Storage Blob Data Contributor ‚Üí acesso ao Storage Account.

‚ö†Ô∏è Aten√ß√£o: a SPN deve ter secret value salvo, n√£o apenas o secret ID.

2. Secrets no GitHub
As seguintes secrets devem ser configuradas no reposit√≥rio (Settings > Secrets and variables > Actions):

DB_KEY ‚Üí chave de acesso para a fonte de dados do banco de dados.

API_KEY ‚Üí chave de acesso para a API de clientes.

ACR_NAME ‚Üí nome do Azure Container Registry (sem o sufixo .azurecr.io).

AKS_NAME ‚Üí nome do cluster Kubernetes (AKS).

STORAGE_ACCOUNT ‚Üí nome da Storage Account.

AZURE_CREDENTIALS ‚Üí credenciais em formato JSON da SPN criada no Azure.

RESOURCE_GROUP ‚Üí nome do Resource Group utilizado.

GH_PAT_TOKEN ‚Üí token pessoal do GitHub, com permiss√µes de leitura/escrita para automa√ß√µes.

3. Configura√ß√µes adicionais
O cluster AKS √© privado, portanto todos os comandos devem ser executados via:

bash
Copiar c√≥digo
az aks command invoke
O node pool do AKS deve utilizar arquitetura AMD (n√£o ARM).

Desabilitar o recurso Network Watcher NG na regi√£o do projeto para evitar conflitos de monitoramento.

4. Workflow Permissions
No reposit√≥rio, configure os Workflow permissions em:
Settings > Actions > General > Workflow permissions

Selecione Read and Write permissions para permitir que os workflows criem e atualizem recursos no Azure.

### 4.4 Execu√ß√£o dos Pipelines de Ingest√£o

A ingest√£o de dados neste projeto √© realizada por meio de **microservi√ßos** executados em um cluster **AKS (Azure Kubernetes Service)**.  
Cada microservi√ßo √© respons√°vel por extrair dados de uma fonte distinta (**Banco de Dados**, **API** e **Arquivos CSV**) e grav√°-los na camada **Raw** do Data Lake.

#### 1. Deploy dos Microservi√ßos
O deploy dos microservi√ßos √© feito via GitHub Actions.  
Ao realizar um **merge** na branch `main`, o pipeline respons√°vel ir√°:

1. **Buildar as imagens** Docker dos microservi√ßos.  
2. **Publicar as imagens** no **Azure Container Registry (ACR)**.  
3. **Deployar as imagens** no cluster **AKS**.

Esse processo √© totalmente automatizado pela esteira de CI/CD configurada no reposit√≥rio.

#### 2. Execu√ß√£o da Ingest√£o
Uma vez que os microservi√ßos estejam em execu√ß√£o no **AKS**, cada um consome sua fonte de dados:

- **Banco de Dados** ‚Üí extrai registros no formato `.json`.  
- **API** ‚Üí coleta dados de clientes e normaliza no mesmo schema.  
- **Arquivos CSV** ‚Üí l√™ e processa arquivos armazenados em diret√≥rios de entrada.  

Todos os dados s√£o enviados para a **camada Raw** do **Azure Storage Account**.

#### 3. Acesso ao Cluster Privado
Como o **AKS** √© privado, os comandos de execu√ß√£o e troubleshooting devem ser feitos usando:

```bash
az aks command invoke \
  --resource-group <RESOURCE_GROUP> \
  --name <AKS_NAME> \
  --command "kubectl get pods -n <namespace>"
4. Observando a Ingest√£o
Para verificar se os microservi√ßos est√£o rodando corretamente:

Validar os pods no cluster:

bash
Copiar c√≥digo
kubectl get pods -n <namespace>
Conferir os logs:

bash
Copiar c√≥digo
kubectl logs <pod_name> -n <namespace>
Validar que os arquivos foram gravados na camada Raw do Data Lake:

bash
Copiar c√≥digo
az storage blob list \
  --account-name <STORAGE_ACCOUNT> \
  --container-name raw \
  --output table
5. Considera√ß√µes
A arquitetura permite que novas fontes sejam adicionadas facilmente criando um novo microservi√ßo e registrando sua imagem no ACR.

O pipeline garante que qualquer atualiza√ß√£o de c√≥digo nos microservi√ßos resulte em uma nova vers√£o sendo automaticamente publicada e executada no AKS.
```

### 4.5 Execu√ß√£o dos Pipelines de Transforma√ß√£o

Ap√≥s a ingest√£o na camada **Raw**, os dados passam por pipelines de transforma√ß√£o no **Databricks**, organizados segundo a **arquitetura medalh√£o** (Bronze ‚Üí Silver ‚Üí Gold).  
Esses pipelines s√£o implementados como **notebooks em Python** e orquestrados via **Databricks Jobs**.

#### 1. Estrutura do Job no Databricks
O Job √© composto por **3 tasks sequenciais**:

1. **Bronze**  
   - Consome dados da camada Raw utilizando o **Auto Loader** do Databricks.  
   - Cria tabelas Delta na camada Bronze.  
   - Garante schema enforcement e versionamento dos dados.  

2. **Silver**  
   - Aplica transforma√ß√µes de limpeza e padroniza√ß√£o.  
   - Realiza o **mascaramento de dados sens√≠veis** (ex.: CPF, cart√£o de cr√©dito).  
   - Cria tabelas Delta refinadas e prontas para an√°lises intermedi√°rias.  

3. **Gold**  
   - Gera tabelas anal√≠ticas e m√©tricas de neg√≥cio (ex.: total de compras, clientes por loja).  
   - Disponibiliza dados prontos para consumo por ferramentas de BI e relat√≥rios.  

#### 2. Execu√ß√£o Manual do Job
Para executar manualmente os pipelines no Databricks:

1. Acesse o **Workspace do Databricks**.  
2. V√° at√© a se√ß√£o **Jobs**.
3. Localize o Job configurado (ex.: `etl-customers`).
4. Clique em **Run Now** para disparar a execu√ß√£o.

#### 3. Execu√ß√£o Automatizada
A execu√ß√£o tamb√©m pode ser disparada automaticamente via **GitHub Actions**:
- Ao atualizar os notebooks no reposit√≥rio, a esteira de CI/CD valida o c√≥digo.  
- Caso aprovado, o pipeline de deploy publica os notebooks no Databricks.  
- O **Databricks CLI** √© ent√£o usado para disparar o Job de transforma√ß√£o.  

#### 4. Monitoramento
Durante a execu√ß√£o do Job, √© poss√≠vel acompanhar:
- **Logs de execu√ß√£o** diretamente no Databricks.  
- Status de cada task (Success / Failed / Running).  
- Hist√≥rico de execu√ß√µes, permitindo auditoria e rastreabilidade.  

#### 5. Considera√ß√µes
- O uso do **Delta Lake** garante versionamento e controle de qualidade.  
- A separa√ß√£o em camadas (Bronze, Silver, Gold) assegura evolu√ß√£o gradual na confiabilidade dos dados.  
- Novas transforma√ß√µes podem ser adicionadas facilmente criando tasks adicionais no Job.

##  üí° 5. Melhorias e Considera√ß√µes Finais

###  5.1 Melhorias Futuras

Apesar de o projeto atender ao objetivo principal de centralizar dados de m√∫ltiplas fontes em um Data Lake estruturado e processado at√© a camada Gold, existem pontos de evolu√ß√£o que podem ser incorporados em vers√µes futuras:

- **Seguran√ßa de Rede (VNet e Private Endpoints):** inclus√£o de Virtual Networks e Private Endpoints para isolar os recursos da Azure e garantir maior seguran√ßa no tr√°fego de dados.  
- **Fluxo de Ambientes (Dev / Pre / Prod):** implementa√ß√£o de m√∫ltiplos ambientes com pipelines de deploy independentes, possibilitando testes e valida√ß√µes antes de subir em produ√ß√£o.  
- **Governan√ßa de Dados:** integra√ß√£o com cat√°logos de dados (ex.: **Unity Catalog**, **Purview**) para melhor rastreabilidade, versionamento e gest√£o de acesso.  
- **Qualidade e Valida√ß√£o de Dados:** uso de ferramentas como **Great Expectations** para validar schemas, detectar anomalias e garantir consist√™ncia antes de promover dados para Silver/Gold.  
- **Orquestra√ß√£o Completa:** adotar um orquestrador dedicado (ex.: **Airflow**, **Prefect**, ou **Azure Data Factory**) para controlar tanto ingest√£o quanto processamento, permitindo maior automa√ß√£o e depend√™ncias entre pipelines.  
- **Observabilidade e Alertas:** centralizar m√©tricas, logs e alertas em ferramentas como **Azure Monitor** ou **Grafana**, possibilitando detec√ß√£o proativa de falhas.  
- **Testes Automatizados:** expandir o uso de testes unit√°rios e de integra√ß√£o para notebooks e microservi√ßos, garantindo maior confiabilidade nas mudan√ßas de c√≥digo.  
- **Custo e Performance:** analisar otimiza√ß√µes de custo (storage tiers, autoscaling de clusters no Databricks) e desempenho (particionamento e otimiza√ß√£o de tabelas Delta).  

###  5.2 Considera√ß√µes Finais

O projeto demonstrou como √© poss√≠vel integrar dados de m√∫ltiplas fontes e formatos, organizando-os em um pipeline escal√°vel e confi√°vel na Azure.  
A solu√ß√£o aplicou boas pr√°ticas de engenharia de dados (arquitetura medalh√£o, uso de Delta Lake, pipelines automatizados) em conjunto com boas pr√°ticas de engenharia de software (microservi√ßos, CI/CD, IaC).  

Embora ainda existam pontos de evolu√ß√£o, a arquitetura atual j√° fornece uma base s√≥lida para ingest√£o, transforma√ß√£o e disponibiliza√ß√£o de dados prontos para an√°lise, podendo ser expandida gradualmente conforme as necessidades de neg√≥cio e requisitos de governan√ßa cres√ßam.

##  üìö 6. Refer√™ncias