# Data Master - Kariman Gomes

<p align="center">
  <img src="assets/images/Data-Master-Logo.png" style="width:800px; height:250px;">
</p>

O reposit√≥rio "DataMaster-DataEngineer-Kariman" apresenta a solu√ß√£o de Engenharia de Dados criada por [Kariman](https://www.linkedin.com/in/kariman-gomes/) como parte do programa Data Master, uma iniciativa da F1rst Santander. <p>

## üìë Sum√°rio

<details>
  <summary>üìå 1. Objetivo do Projeto</summary>

  - [In√≠cio R√°pido](#inicio-rapido)

</details>

<details>
  <summary>üèóÔ∏è 2. Arquitetura de Solu√ß√£o</summary>

  - [Vis√£o Geral](#visao-geral)
  - [Diagrama da Arquitetura de Solu√ß√£o](#diagrama-da-arquitetura-de-solucao)
  - [Componentes Principais](#componentes-principais)
  - [Caracter√≠sticas Essenciais do Projeto](#caracteristicas-essenciais-do-projeto)

</details>

<details>
  <summary>‚öôÔ∏è 3. Arquitetura T√©cnica</summary>

  - [Vis√£o Geral T√©cnica](#visao-geral-tecnica)
  - [Descri√ß√£o do Fluxo de Dados](#descricao-do-fluxo-de-dados)
  - [Modelagem e Estrutura do Data Lake](#modelagem-e-estrutura-do-data-lake)
  - [Tecnologias e Servi√ßos Utilizados](#tecnologias-e-servicos-utilizados)
  - [Infraestrutura como C√≥digo](#infraestrutura-como-codigo)
  - [Orquestra√ß√£o de Pipelines](#orquestracao-de-pipelines)
  - [Extra√ß√£o e Ingest√£o de Dados](#extracao-e-ingestao-de-dados)
  - [Armazenamento de Dados](#armazenamento-de-dados)
  - [Processamento e Transforma√ß√£o dos Dados](#processamento-e-transformacao-dos-dados)
  - [Qualidade e Valida√ß√£o de Dados](#qualidade-e-validacao-de-dados)
  - [Mascaramento e Seguran√ßa dos Dados](#mascaramento-e-seguranca-dos-dados)
  - [Observabilidade e Monitoramento](#observabilidade-e-monitoramento)
  - [Escalabilidade e Desempenho](#escalabilidade-e-desempenho)

</details>

<details>
  <summary>üöÄ 4. Guia de Configura√ß√£o e Execu√ß√£o</summary>

  - [Pr√©-requisitos](#pre-requisitos)
  - [Configura√ß√£o da Infraestrutura](#configuracao-da-infraestrutura)
  - [Configura√ß√£o de Credenciais e Acessos](#configuracao-de-credenciais-e-acessos)
  - [Execu√ß√£o dos Pipelines de Ingest√£o](#execucao-dos-pipelines-de-ingestao)
  - [Execu√ß√£o dos Pipelines de Transforma√ß√£o](#execucao-dos-pipelines-de-transformacao)
  - [Execu√ß√£o da Integra√ß√£o com o CRM](#execucao-da-integracao-com-o-crm)

</details>

<details>
  <summary>üí° 5. Melhorias e Considera√ß√µes Finais</summary>

  - [Melhorias Futuras](#melhorias-futuras)
  - [Considera√ß√µes Finais](#consideracoes-finais)

</details>

<details>
  <summary>üí∞ 6. Custos do Projeto</summary>
</details>

<details>
  <summary>üìö 7. Refer√™ncias</summary>
</details>

##  üìå 1. Objetivo do Projeto

Este projeto busca integrar dados de clientes espalhados em diferentes fontes e formatos, que dificultam a integra√ß√£o, padroniza√ß√£o e an√°lise unificada em um √∫nico reposit√≥rio estruturado, permitindo que os dados sejam processados, transformados e analisados de maneira consistente e confi√°vel, seguindo boas pr√°ticas de engenharia de dados e de software.

### 1.1 Problema

Atualmente, os dados de clientes encontram-se distribu√≠dos em diversas fontes, como bancos de dados, APIs e arquivos CSV. Essa fragmenta√ß√£o gera dificuldades como:  
- Redund√¢ncia e inconsist√™ncia entre registros.  
- Dificuldade de padroniza√ß√£o e integra√ß√£o dos dados.  
- Falta de rastreabilidade e governan√ßa.  
- Barreiras para an√°lises unificadas e confi√°veis.

### 1.2 Solu√ß√£o

A solu√ß√£o proposta √© construir um pipeline de dados baseado na arquitetura medalh√£o (Raw ‚Üí Bronze ‚Üí Silver ‚Üí Gold), que:  
- Faz a ingest√£o dados de m√∫ltiplas fontes por meio de microservi√ßos.  
- Centraliza e organiza as informa√ß√µes em um Data Lake na Azure.
- Processa e transforma os dados utilizando Databricks e Delta Lake. 
- Aplica limpeza, padroniza√ß√£o e mascaramento de dados sens√≠veis.  
- Disponibiliza camadas de dados confi√°veis e prontas para consumo anal√≠tico.

##  üèóÔ∏è 2. Arquitetura de Solu√ß√£o

### 2.1 Vis√£o Geral

A arquitetura do case foi projetada para integrar dados de diferentes fontes e garantir que eles estejam dispon√≠veis de forma consistente e confi√°vel para an√°lise. Os dados s√£o ingeridos de tr√™s origens distintas (banco de dados, API e arquivos CSV) por meio de microservi√ßos, processados e transformados em camadas de dados estruturadas (Bronze ‚Üí Silver ‚Üí Gold) e armazenados em um Data Lake na Azure.

O fluxo de dados segue a arquitetura medalh√£o, garantindo que cada camada tenha dados com n√≠veis crescentes de qualidade e consist√™ncia, desde a ingest√£o bruta at√© a camada pronta para an√°lise.

### 2.2 Diagrama da Arquitetura de Solu√ß√£o

![Figura 1 ‚Äî Arquitetura de Solu√ß√£o](assets/images/Arquitetura-Pipeline-de-Dados.png)

*Figura 1 ‚Äî Arquitetura de solu√ß√£o em alto n√≠vel, mostrando ingest√£o, processamento e armazenamento das camadas do Data Lake.*

### 2.3 Componentes Principais

- **Microservi√ßos de Ingest√£o**: Cada microservi√ßo √© respons√°vel por extrair dados de uma fonte espec√≠fica e grav√°-los na camada raw do Data Lake. Eles podem ser detalhados posteriormente quanto √† linguagem, endpoints e bibliotecas utilizadas.  
- **Data Lake (Azure Storage Account)**: Armazena os dados em diferentes camadas de processamento (Raw, Bronze, Silver e Gold), seguindo a arquitetura medalh√£o e permitindo rastreabilidade e governan√ßa.  
- **Databricks**: realiza o processamento e a transforma√ß√£o dos dados. A camada Bronze utiliza o Auto Loader para ingest√£o automatizada, a camada Silver aplica limpeza, padroniza√ß√£o e mascaramento de dados, e a camada Gold gera tabelas prontas para an√°lise.  
- **Infraestrutura como C√≥digo (Terraform)**: Provisiona todos os recursos necess√°rios, incluindo AKS, Storage Account, Databricks e demais componentes da arquitetura.  
- **Automa√ß√£o (GitHub Actions)**: Gerencia a cria√ß√£o, o deploy dos microservi√ßos e verifica√ß√£o de qualidade de c√≥digo.

## ‚öôÔ∏è 3. Arquitetura T√©cnica

### 3.1 Vis√£o Geral T√©cnica

O projeto utiliza uma arquitetura em nuvem na Azure, combinando pr√°ticas de engenharia de dados e engenharia de software. A infraestrutura √© provisionada via Terraform e composta por microservi√ßos para ingest√£o, Data Lake para armazenamento estruturado, Databricks para processamento e pipelines automatizados com GitHub Actions. O objetivo t√©cnico √© permitir a ingest√£o, transforma√ß√£o e disponibiliza√ß√£o de dados confi√°veis e padronizados em um ambiente reproduz√≠vel e escal√°vel.

### 3.2 Descri√ß√£o do Fluxo de Dados

1. **Ingest√£o**: Os microservi√ßos consomem dados das fontes (Banco de Dados, API, Arquivos) e gravam na camada raw do Data Lake. 
2. **Bronze**: Databricks l√™ os dados da raw e cria tabelas Delta, mantendo a integridade das informa√ß√µes.  
3. **Silver**: Aplica transforma√ß√µes de limpeza, padroniza√ß√£o e mascaramento de dados sens√≠veis.  
4. **Gold**: Gera tabelas prontas para consumo por ferramentas de BI ou an√°lises avan√ßadas.  

### 3.3 Tecnologias e Servi√ßos Utilizados

- **Resource Group**: Agrupa todos os recursos provisionados na Azure. 
- **Storage Account**: Armazenamento do Data Lake com camadas Raw, Bronze, Silver e Gold.
- **ACR (Azure Container Service)**: Reposit√≥rio para armazenar e versionar as imagens Docker dos microservi√ßos.  
- **AKS (Azure Kubernetes Service)**: Execu√ß√£o dos microservi√ßos de ingest√£o.
- **Databricks**: Processamento e transforma√ß√£o dos dados.
- **Delta Lake**: Garantia de consist√™ncia, versionamento e ACID nas tabelas.
- **Auto Loader**: Respons√°vel por ingest√£o cont√≠nua dos dados da camada Raw para a Bronze.
- **Terraform**: Provisionamento de toda a infraestrutura na Azure.
- **GitHub Actions**: Automa√ß√£o de cria√ß√£o de infraestrutura, deploy de microservi√ßos e execu√ß√£o de jobs.

### 3.4 Infraestrutura como C√≥digo

![Figura 2 ‚Äî Infraestrutura CI/CD](assets/images/Arquitetura-Infrastructure-CI-CD.png)  

*Figura 2 ‚Äî Arquitetura de infraestrutura com Terraform e principais recursos provisionados na Azure.*

Toda a infraestrutura do projeto √© criada com uso de **Terraform**, que tamb√©m salva o estado das cria√ß√µes para permitir atualiza√ß√µes ou exclus√£o da infraestrutura.

### Valida√ß√µes (CI)
- **Check Github Token**: Antes da cria√ß√£o dos recursos, √© verificado se o *Personal Access Token* do GitHub est√° criado e configurado com os acessos necess√°rios.
- **Check Azure Role Assignments**: Valida se o *Service Principal* necess√°rio para a cria√ß√£o dos recursos est√° configurado corretamente com as permiss√µes adequadas.  

### Cria√ß√£o de Recursos (CD)
- **Resource Group**: Respons√°vel por armazenar os recursos.  
- **Azure Container Registry (ACR)**: Armazena as imagens dos microservi√ßos.  
- **Azure Kubernetes Service (AKS)**: Cluster respons√°vel pela execu√ß√£o dos microservi√ßos.  
- **Storage Account**: Armazenamento das tabelas de dados.  
- **Storage Containers**: Containers espec√≠ficos para as camadas de dados Raw, Bronze, Silver e Gold.  
- **Databricks**: Utilizado para o processamento de dados, desde a camada Raw at√© a camada Gold.

### 3.5 GitHub Actions

O GitHub Actions √© respons√°vel por orquestrar e automatizar todas as etapas do projeto, incluindo:

- **Provisionamento de Infraestrutura**: executa o Terraform para cria√ß√£o e atualiza√ß√£o de recursos na Azure.  
- **Deploy de Microservi√ßos**: build e publica√ß√£o de imagens Docker no ACR, seguido de deploy no AKS.  
- **Transforma√ß√£o de Dados**: integra√ß√£o com Databricks para execu√ß√£o dos pipelines de Bronze, Silver e Gold.  

#### Resumo de Execu√ß√£o (Summary)
Cada execu√ß√£o de workflow gera automaticamente um **summary** dentro do GitHub Actions, contendo:  
- Recursos criados/atualizados pelo Terraform.  
- Status de execu√ß√£o das etapas (Infra, Ingest√£o, Transforma√ß√£o).  
- Outputs importantes do Terraform.  
- Links para logs detalhados de cada job.  

**Colocar imagem aqui dos summaries depois**

Esse resumo facilita a **auditoria** e permite acompanhar rapidamente o resultado das execu√ß√µes sem precisar navegar em todos os logs.

#### Fluxo dos Workflows
1. **Infraestrutura** ‚Üí Provisionamento completo via Terraform.  
2. **Ingest√£o** ‚Üí Deploy autom√°tico de microservi√ßos no AKS.  
3. **Transforma√ß√£o** ‚Üí Execu√ß√£o dos notebooks no Databricks.  
4. **Exclus√£o** ‚Üí Workflow espec√≠fico para destrui√ß√£o segura da infraestrutura.  

Todos os workflows podem ser acionados manualmente (`workflow_dispatch`) ou automaticamente via `push` na branch principal.

### 3.6 Orquestra√ß√£o de Pipelines

Orquestra√ß√£o ainda em desenvolvimento

### 3.7 Extra√ß√£o e Ingest√£o de Dados

![Figura 3 ‚Äî Microservi√ßos de Ingest√£o](assets/images/Arquitetura-Microservices-CI-CD.png)  

*Figura 3 ‚Äî Arquitetura de CI/CD dos microservi√ßos de ingest√£o: extra√ß√£o de dados, build, deploy no AKS.*

A ingest√£o dos dados √© realizada por **microservi√ßos** desenvolvidos para cada fonte de dados (Banco de Dados, API e Arquivos). Esses microservi√ßos s√£o empacotados em cont√™ineres e executados em um **Azure Kubernetes Service (AKS)**.

O fluxo segue os seguintes passos:  
1. Cada microservi√ßo se conecta √† sua respectiva fonte.  
2. Os dados s√£o extra√≠dos no formato de origem (JSON ou CSV).  
3. Os microservi√ßos gravam os dados diretamente na **camada Raw** do Data Lake, em pastas espec√≠ficos.  

Caracter√≠sticas principais:
- **Independ√™ncia**: Cada microservi√ßo √© respons√°vel por uma fonte, facilitando manuten√ß√£o e evolu√ß√£o.  
- **Padroniza√ß√£o**: Apesar das diferentes origens, os dados seguem o mesmo schema.  
- **Escalabilidade**: O uso do AKS permite aumentar ou reduzir r√©plicas de ingest√£o conforme a demanda.  
- **Automa√ß√£o**: Pipelines no GitHub Actions garantem que os microservi√ßos sejam validados, constru√≠dos e implantados automaticamente no cluster.

#### Estrutura
microservice_name/
‚îú‚îÄ‚îÄ helm/               # Helm chart para deploy no AKS
‚îú‚îÄ‚îÄ src/
‚îÇ ‚îú‚îÄ‚îÄ application/      # Regras de neg√≥cio da aplica√ß√£o
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ helpers/        # Fun√ß√µes utilit√°rias
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ services/       # Servi√ßos de orquestra√ß√£o da l√≥gica
‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ validators/     # Valida√ß√µes de entrada e regras espec√≠ficas
‚îÇ ‚îú‚îÄ‚îÄ domain/           # Defini√ß√µes de dom√≠nio
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ exceptions/     # Classes de exce√ß√µes espec√≠ficas
‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ ports/          # Interfaces para comunica√ß√£o entre camadas
‚îÇ ‚îú‚îÄ‚îÄ infrastructure/   # Implementa√ß√µes concretas
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ authentication/ # Autentica√ß√£o e seguran√ßa
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ config/         # Configura√ß√µes de ambiente
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ logging/        # Logs e monitoramento
‚îÇ ‚îÇ ‚îú‚îÄ‚îÄ ingestion/      # Conectores e ingest√£o de dados
‚îÇ ‚îÇ ‚îî‚îÄ‚îÄ storage/        # Intera√ß√£o com o Data Lake
‚îÇ ‚îî‚îÄ‚îÄ interfaces/       # Pontos de entrada
‚îÇ ‚îî‚îÄ‚îÄ main.py           # API/CLI principal do microservi√ßo
‚îú‚îÄ‚îÄ tests/              # Testes unit√°rios e de integra√ß√£o
‚îú‚îÄ‚îÄ Dockerfile          # Defini√ß√£o da imagem Docker
‚îú‚îÄ‚îÄ Makefile            # Defini√ß√£o da imagem Docker
‚îú‚îÄ‚îÄ requirements.txt    # Depend√™ncias do servi√ßo
‚îú‚îÄ‚îÄ setup.py            # Defini√ß√£o da imagem Docker
‚îî‚îÄ‚îÄ VERSION             # Usado para versionamento do Microservi√ßo

A organiza√ß√£o segue princ√≠pios da Clean Architecture, garantindo separa√ß√£o de responsabilidades e facilidade de manuten√ß√£o:
- **domain** ‚Üí Regras de neg√≥cio puras, interfaces (ports) e exce√ß√µes.
- **application** ‚Üí Casos de uso, servi√ßos, validadores e helpers.
- **infrastructure** ‚Üí Autentica√ß√£o, logging, ingest√£o e persist√™ncia.
- **interfaces** ‚Üí Pontos de entrada do servi√ßo (ex.: main.py).
- **tests** ‚Üí Testes unit√°rios e de integra√ß√£o.
- **helm** ‚Üí Manifests para deploy no AKS.

Arquivos de Configura√ß√£o:
- **.dockerignore** -> Exclui arquivos desnecess√°rios no build Docker.  
- **.flake8** -> Regras de lint para garantir padr√£o de c√≥digo. 
- **.gitignore** -> Define arquivos ignorados no versionamento.
- **.pre-commit-config.yaml** -> Hooks para valida√ß√µes autom√°ticas antes do commit.
- **pytest.ini** -> Configura√ß√µes para execu√ß√£o dos testes com Pytest.

### 3.8 Armazenamento de Dados

O armazenamento dos dados √© realizado em uma **Azure Storage Account**, estruturada segundo a arquitetura medalh√£o.
Cada camada possui um **container dedicado**, garantindo organiza√ß√£o e isolamento entre os est√°gios do pipeline:

- **Raw**: Camada onde os microservi√ßos depositam os dados ingeridos, preservando-os no formato original.  
- **Bronze**: Camada onde o Databricks cria tabelas Delta a partir da Raw, garantindo rastreabilidade e hist√≥rico.  
- **Silver**: Camada que cont√©m dados limpos, padronizados e com mascaramento de informa√ß√µes sens√≠veis.  
- **Gold**: Camada final com dados prontos para consumo em an√°lises e dashboards. 

### 3.9 Processamento e Transforma√ß√£o dos Dados

![Figura 4 ‚Äî Data Processing CI](assets/images/Arquitetura-Data-Processing-CI.png)

*Figura 4 ‚Äî Arquitetura de processamento e transforma√ß√£o de dados em Databricks, seguindo a arquitetura medalh√£o.*

O processamento dos dados √© realizado no **Databricks**, utilizando **Microservi√ßos Python** organizados em tarefas dentro de um **Job**.
Cada job √© dividido em tr√™s etapas principais, alinhadas √† arquitetura medalh√£o:

1. **Bronze**  
   - Leitura dos dados da camada Raw por meio do **Auto Loader** do Databricks.  
   - Cria√ß√£o de tabelas **Delta Lake**, preservando os dados ingeridos com hist√≥rico e versionamento.  
   - Registro de metadados iniciais para rastreabilidade.  

2. **Silver**  
   - Aplica√ß√£o de regras de **limpeza e padroniza√ß√£o** (ex.: normaliza√ß√£o de formatos, remo√ß√£o de inconsist√™ncias).  
   - **Mascaramento de informa√ß√µes sens√≠veis**, como CPF e n√∫mero de cart√£o de cr√©dito.  
   - Enriquecimento dos dados quando necess√°rio.  

3. **Gold**  
   - Estrutura√ß√£o dos dados em modelos anal√≠ticos.  
   - Prepara√ß√£o das tabelas para consumo em ferramentas de BI e relat√≥rios.  
   - Disponibiliza√ß√£o de dados consistentes e confi√°veis para an√°lise.

#### Estrutura
processing_job/  
‚îú‚îÄ‚îÄ src/  
‚îÇ   ‚îú‚îÄ‚îÄ config/       # Configura√ß√µes do pipeline (par√¢metros, schemas, paths)  
‚îÇ   ‚îú‚îÄ‚îÄ modules/      # M√≥dulos principais de transforma√ß√£o (Bronze, Silver ou Gold)  
‚îÇ   ‚îú‚îÄ‚îÄ utils/        # Fun√ß√µes utilit√°rias reutiliz√°veis  
‚îÇ   ‚îî‚îÄ‚îÄ main.py       # Script principal do job executado no Databricks  
‚îú‚îÄ‚îÄ tests/            # Testes unit√°rios e de integra√ß√£o  
‚îú‚îÄ‚îÄ requirements.txt  # Depend√™ncias do job

Arquivos de Configura√ß√£o
- **.flake8** ‚Üí Regras de lint para garantir padr√£o de c√≥digo.
- **.gitignore** -> Define arquivos ignorados no versionamento.
- **.pre-commit-config.yaml** ‚Üí Hooks para valida√ß√µes autom√°ticas antes do commit.  
- **pytest.ini** ‚Üí Configura√ß√µes para execu√ß√£o dos testes com Pytest.

### 3.10 Qualidade e Valida√ß√£o de Dados

A qualidade dos dados √© garantida a partir da camada **Silver**, onde s√£o aplicadas regras de valida√ß√£o e consist√™ncia antes de disponibilizar as informa√ß√µes para consumo.  
O processo combina verifica√ß√µes automatizadas e padroniza√ß√µes implementadas nos notebooks do Databricks.

Principais valida√ß√µes aplicadas:  
- **Integridade de schema**: checagem se os dados seguem o schema esperado.  
- **Valores obrigat√≥rios**: verifica√ß√£o de colunas cr√≠ticas que n√£o podem estar nulas (ex.: `customer_id`, `cpf`).  
- **Formatos v√°lidos**: confer√™ncia de padr√µes, como CPF v√°lido ou formato correto de e-mail.  
- **Valores de dom√≠nio**: valida√ß√£o de atributos contra listas pr√©-definidas (ex.: localiza√ß√£o de lojas).  

Ferramentas e pr√°ticas:  
- **Delta Lake** para versionamento e rollback em caso de ingest√£o incorreta.  
- **Microservi√ßo Python no Databricks** com fun√ß√µes de valida√ß√£o.    

**Benef√≠cios principais**:  
- Evita propaga√ß√£o de dados inconsistentes para as camadas anal√≠ticas.  
- Garante confiabilidade e consist√™ncia para relat√≥rios e dashboards.  
- Facilita auditoria e rastreabilidade em caso de erros de ingest√£o ou transforma√ß√£o.

### 3.11 Mascaramento e Seguran√ßa dos Dados

O projeto adota pr√°ticas de **seguran√ßa e privacidade** para proteger informa√ß√µes sens√≠veis dos clientes durante o ciclo de vida dos dados.  
O foco principal est√° no **mascaramento de dados pessoais**, realizado na transi√ß√£o da camada **Bronze ‚Üí Silver**.

#### Mascaramento de Dados
- **CPF**: substitui√ß√£o parcial dos d√≠gitos, preservando apenas os √∫ltimos 3 para rastreabilidade.  
- **N√∫mero de cart√£o de cr√©dito**: oculta√ß√£o de todos os d√≠gitos, exceto os 4 √∫ltimos.  
- **E-mail**: ofusca√ß√£o parcial do endere√ßo, mantendo o dom√≠nio vis√≠vel.  

Essas transforma√ß√µes s√£o aplicadas atrav√©s dos **Microservi√ßos Python**, garantindo que os dados sens√≠veis n√£o avancem para a camada Gold.

#### Seguran√ßa de Armazenamento e Acesso
- **Azure Storage Account**: Controle de permiss√µes via *role-based access control* (RBAC).  
- **Segrega√ß√£o por camadas**: Cada container (Raw, Bronze, Silver, Gold) possui pol√≠ticas de acesso espec√≠ficas.  
- **GitHub Actions**: Uso de *secrets* para armazenar credenciais de forma segura.  
- **Service Principals**: Autentica√ß√£o entre servi√ßos com permiss√µes m√≠nimas necess√°rias.  

**Benef√≠cios principais**:  
- Prote√ß√£o de informa√ß√µes sens√≠veis em conformidade com boas pr√°ticas de governan√ßa.  
- Redu√ß√£o de riscos em auditorias e conformidade regulat√≥ria (LGPD).  
- Garantia de que dados anal√≠ticos n√£o exponham informa√ß√µes pessoais desnecess√°rias.  

### 3.12 Observabilidade e Monitoramento

O projeto prev√™ mecanismos de **observabilidade e monitoramento** para acompanhar a execu√ß√£o dos pipelines, identificar falhas rapidamente e garantir confiabilidade no fluxo de dados.

#### Logs e M√©tricas
- **Microservi√ßos (AKS)**: gera√ß√£o de logs de execu√ß√£o e falhas, que podem ser integrados ao **Azure Monitor** ou **Application Insights**.  
- **Databricks Jobs**: registro autom√°tico de logs de execu√ß√£o, status de tarefas e m√©tricas de processamento.  
- **GitHub Actions**: logs detalhados de cada etapa de CI/CD, permitindo auditoria das execu√ß√µes.  

#### Alertas
- Configura√ß√£o de alertas no **Azure Monitor** para falhas em microservi√ßos ou indisponibilidade de recursos.  
- Alertas no **Databricks** para jobs que falharem ou ultrapassarem o tempo limite de execu√ß√£o.  

#### Observabilidade de Dados
- M√©tricas de volume de dados ingeridos em cada fonte.  
- Taxa de erros de ingest√£o ou registros inv√°lidos.  
- Possibilidade futura de integra√ß√£o com ferramentas de **Data Lineage** para rastreabilidade ponta a ponta.  

**Benef√≠cios principais**:  
- Redu√ß√£o do tempo de detec√ß√£o e resposta a falhas.  
- Garantia de confiabilidade e disponibilidade dos pipelines.  
- Maior transpar√™ncia sobre a sa√∫de do ecossistema de dados.  

### 3.13 Escalabilidade e Desempenho

O projeto foi estruturado para suportar aumento de volume de dados e crescimento no n√∫mero de fontes, mantendo efici√™ncia e confiabilidade no processamento.

#### Escalabilidade
- **Microservi√ßos em AKS**: permitem escalar horizontalmente, aumentando ou reduzindo r√©plicas conforme a demanda de ingest√£o.  
- **Databricks**: suporte a clusters escal√°veis sob demanda, otimizando custo e performance no processamento das camadas Bronze, Silver e Gold.  
- **Storage Account**: arquitetura baseada em containers independentes para cada camada de dados, possibilitando expans√£o sem necessidade de reestrutura√ß√£o.  

#### Desempenho
- **Delta Lake**: garante performance em consultas e manipula√ß√£o de grandes volumes de dados por meio de otimiza√ß√µes internas (indexa√ß√£o, caching e compacta√ß√£o de arquivos).  
- **Auto Loader**: possibilita ingest√£o cont√≠nua e eficiente dos dados da Raw para a Bronze, reduzindo o tempo de lat√™ncia.  
- **Transforma√ß√µes distribu√≠das no Databricks**: uso de processamento paralelo para melhorar a velocidade em opera√ß√µes de limpeza e padroniza√ß√£o.

#### Otimiza√ß√£o de Escrita no Delta Lake  

O cluster Databricks est√° configurado com par√¢metros de otimiza√ß√£o para melhorar a performance de escrita e leitura de dados em tabelas Delta:  

- `spark.databricks.delta.optimizeWrite.enabled = true` ‚Üí Habilita **optimize write**, que reduz o n√∫mero de pequenos arquivos gerados, consolidando-os automaticamente durante a escrita.  
- `spark.databricks.delta.autoCompact.enabled = true` ‚Üí Ativa a **auto compacta√ß√£o**, que combina arquivos pequenos em arquivos maiores de forma cont√≠nua, melhorando a performance de leitura e queries anal√≠ticas.

**Benef√≠cios principais**:  
- Capacidade de lidar com aumento no volume e variedade de dados.  
- Redu√ß√£o da lat√™ncia entre ingest√£o e disponibiliza√ß√£o dos dados anal√≠ticos.  
- Otimiza√ß√£o de custos ao escalar recursos somente quando necess√°rio.

### 3.14 Metodologia de Desenvolvimento

O ciclo de desenvolvimento do projeto segue a estrat√©gia de **GitFlow** para organiza√ß√£o e rastreabilidade do c√≥digo.  

Principais pr√°ticas adotadas:
- **Branch main** ‚Üí sempre est√°vel, representa a vers√£o de produ√ß√£o.  
- **Branch develop** ‚Üí concentra as novas features e integra√ß√µes em andamento.  
- **Feature branches** ‚Üí criadas a partir de `develop` para desenvolvimento de funcionalidades espec√≠ficas.  
- **Release branches** ‚Üí usadas para preparar vers√µes est√°veis antes de ir para produ√ß√£o.  
- **Hotfix branches** ‚Üí permitem corre√ß√µes r√°pidas diretamente na `main`.

**Adicionar imagens das Branchs**

##  üöÄ 4. Guia de Configura√ß√£o e Execu√ß√£o

### 4.1 Pr√©-requisitos

Antes de configurar e executar o projeto, √© necess√°rio garantir que o ambiente possua os seguintes pr√©-requisitos:

#### Identidade, Permiss√µes, Credenciais e Acessos
- **Service Principal (SPN)** criado previamente.  
- **Secret da SPN** configurado (*Secret Value*, n√£o o *Secret ID*).  
- A **SPN** precisa ter permiss√µes na assinatura da Azure:  
  - *Contributor*  
  - *User Access Administrator*

#### GitHub Actions
- **Personal Access Token (PAT)** do GitHub criado e salvo nas *Secrets* do reposit√≥rio com permiss√µes adequadas.  

#### Secrets obrigat√≥rias no GitHub
As seguintes *secrets* devem estar configuradas no reposit√≥rio antes da execu√ß√£o de qualquer fluxo de cria√ß√£o:  

- `DB_KEY` ‚Üí Para conex√£o com o Database.  
- `API_KEY` ‚Üí Para conex√£o com a API.  
- `AZURE_CREDENTIALS` ‚Üí Credenciais para conex√£o na Azure, seguindo o seguinte modelo JSON:  
  
  ```json
  {
    "clientId": "", 
    "clientSecret": "", 
    "subscriptionId": "", 
    "tenantId": "" 
  }
  ```

- `GH_PAT_TOKEN` ‚Üí Token criado no GitHub, com as seguintes permiss√µes:

repo ‚Üí Full control of private repositories
repo:status ‚Üí Access commit status
repo_deployment ‚Üí Access deployment status
public_repo ‚Üí Access public repositories
repo:invite ‚Üí Access repository invitations
security_events ‚Üí Read and write security events
workflow ‚Üí Update GitHub Action workflows
write:packages ‚Üí Upload packages to GitHub Package Registry
read:packages ‚Üí Download packages from GitHub Package Registry
admin:org ‚Üí Full control of orgs and teams, read and write org projects
write:org ‚Üí Read and write org and team membership, read and write org projects
read:org ‚Üí Read org and team membership, read org projects
manage_runners:org ‚Üí Manage org runners and runner groups

### 4.2 Configura√ß√£o da Infraestrutura

1. **Criar reposit√≥rio a partir do template**  
   - Clique no bot√£o **"Use this template"** no reposit√≥rio original.  
   - Crie seu pr√≥prio reposit√≥rio a partir dele.  
   - Fa√ßa o clone do **seu reposit√≥rio rec√©m-criado**:  
     ```bash
     git clone https://github.com/<usuario>/<novo-repo>.git
     cd <novo-repo>
     ```
    **OBS**: Colocar aqui imagem dos recursos criados na Azure

2. **Configurar as *secrets* no GitHub**  
   - Defina todas as *secrets* obrigat√≥rias listadas na se√ß√£o **Pr√©-requisitos**.  
   - Certifique-se de que as **Workflow permissions** estejam configuradas como *Read and Write*.

   **OBS**: Colocar aqui imagem dos recursos criados na Azure

3. **Executar o workflow de cria√ß√£o da infraestrutura**  
   - O workflow do GitHub Actions respons√°vel pela cria√ß√£o deve ser acionado manualmente (`workflow_dispatch`) ou via push no branch principal.  
   - Esse pipeline realiza:  
     - Valida√ß√£o de credenciais e permiss√µes.  
     - Cria√ß√£o/atualiza√ß√£o do **Resource Group**.  
     - Provisionamento do **Storage Account** com containers (Raw, Bronze, Silver, Gold).  
     - Cria√ß√£o do **Azure Container Registry (ACR)**.  
     - Deploy do **Azure Kubernetes Service (AKS)**.  
     - Configura√ß√£o do **Azure Databricks**.

     **OBS**: Colocar aqui imagem dos recursos criados na Azure

4. **Validar a implanta√ß√£o**  
   - Confirme que todos os recursos foram criados no **Resource Group** especificado.  
   
   **OBS**: Colocar aqui imagem dos recursos criados na Azure

---

**Observa√ß√µes**  
- O **Terraform** utiliza *Remote State* armazenado nos *artifacts* do GitHub Actions, permitindo atualiza√ß√µes e destrui√ß√£o da infraestrutura de forma segura.
- Existe um workflow espec√≠fico para exclus√£o completa da infraestrutura.
  
  **OBS**: Colocar aqui imagem dos recursos criados na Azure

### 4.4 Execu√ß√£o dos Pipelines de Ingest√£o

A ingest√£o de dados neste projeto √© realizada por meio de **microservi√ßos** executados em um cluster **AKS (Azure Kubernetes Service)**.  
Cada microservi√ßo √© respons√°vel por extrair dados de uma fonte distinta (**Banco de Dados**, **API** e **Arquivos CSV**) e grav√°-los na camada **Raw** do Data Lake.

#### 1. Deploy dos Microservi√ßos
O deploy dos microservi√ßos √© feito via GitHub Actions.  
Ao realizar um **merge** na branch `main`, o pipeline respons√°vel ir√°:

1. **Buildar as imagens** Docker dos microservi√ßos.  
2. **Publicar as imagens** no **Azure Container Registry (ACR)**.  
3. **Deployar as imagens** no cluster **AKS**.

Esse processo √© totalmente automatizado pela esteira de CI/CD configurada no reposit√≥rio.

#### 2. Execu√ß√£o da Ingest√£o
Uma vez que os microservi√ßos estejam em execu√ß√£o no **AKS**, cada um consome sua fonte de dados:

- **Banco de Dados** ‚Üí extrai registros no formato `.json`.  
- **API** ‚Üí coleta dados de clientes e normaliza no mesmo schema.  
- **Arquivos CSV** ‚Üí l√™ e processa arquivos armazenados em diret√≥rios de entrada.  

Todos os dados s√£o enviados para a **camada Raw** do **Azure Storage Account**.

#### 3. Acesso ao Cluster Privado
Como o **AKS** √© privado, os comandos de execu√ß√£o e troubleshooting devem ser feitos usando:

5. Considera√ß√µes
A arquitetura permite que novas fontes sejam adicionadas facilmente criando um novo microservi√ßo e registrando sua imagem no ACR.

O pipeline garante que qualquer atualiza√ß√£o de c√≥digo nos microservi√ßos resulte em uma nova vers√£o sendo automaticamente publicada e executada no AKS.

### 4.5 Execu√ß√£o dos Pipelines de Transforma√ß√£o

Ap√≥s a ingest√£o na camada **Raw**, os dados passam por pipelines de transforma√ß√£o no **Databricks**, organizados segundo a **arquitetura medalh√£o** (Bronze ‚Üí Silver ‚Üí Gold).  
Esses pipelines s√£o implementados como **notebooks em Python** e orquestrados via **Databricks Jobs**.

#### 1. Estrutura do Job no Databricks
O Job √© composto por **3 tasks sequenciais**:

1. **Bronze**  
   - Consome dados da camada Raw utilizando o **Auto Loader** do Databricks.  
   - Cria tabelas Delta na camada Bronze.  
   - Garante schema enforcement e versionamento dos dados.  

2. **Silver**  
   - Aplica transforma√ß√µes de limpeza e padroniza√ß√£o.  
   - Realiza o **mascaramento de dados sens√≠veis** (ex.: CPF, cart√£o de cr√©dito).  
   - Cria tabelas Delta refinadas e prontas para an√°lises intermedi√°rias.  

3. **Gold**  
   - Gera tabelas anal√≠ticas e m√©tricas de neg√≥cio (ex.: total de compras, clientes por loja).  
   - Disponibiliza dados prontos para consumo por ferramentas de BI e relat√≥rios.  

#### 2. Execu√ß√£o Manual do Job
Para executar manualmente os pipelines no Databricks:

1. Acesse o **Workspace do Databricks**.  
2. V√° at√© a se√ß√£o **Jobs**.
3. Localize o Job configurado (ex.: `etl-customers`).
4. Clique em **Run Now** para disparar a execu√ß√£o.

#### 3. Execu√ß√£o Automatizada
A execu√ß√£o tamb√©m pode ser disparada automaticamente via **GitHub Actions**:
- Ao atualizar os notebooks no reposit√≥rio, a esteira de CI/CD valida o c√≥digo.  
- Caso aprovado, o pipeline de deploy publica os notebooks no Databricks.  
- O **Databricks CLI** √© ent√£o usado para disparar o Job de transforma√ß√£o.  

#### 4. Monitoramento
Durante a execu√ß√£o do Job, √© poss√≠vel acompanhar:
- **Logs de execu√ß√£o** diretamente no Databricks.  
- Status de cada task (Success / Failed / Running).  
- Hist√≥rico de execu√ß√µes, permitindo auditoria e rastreabilidade.  

#### 5. Considera√ß√µes
- O uso do **Delta Lake** garante versionamento e controle de qualidade.  
- A separa√ß√£o em camadas (Bronze, Silver, Gold) assegura evolu√ß√£o gradual na confiabilidade dos dados.  
- Novas transforma√ß√µes podem ser adicionadas facilmente criando tasks adicionais no Job.

##  üí° 5. Melhorias e Considera√ß√µes Finais

###  5.1 Melhorias Futuras

- **Seguran√ßa de Rede (VNet e Private Endpoints):** inclus√£o de Virtual Networks e Private Endpoints para isolar os recursos da Azure e garantir maior seguran√ßa no tr√°fego de dados. 
- **Fluxo de Ambientes (Dev / Pre / Prod):** implementa√ß√£o de m√∫ltiplos ambientes com pipelines de deploy independentes, possibilitando testes e valida√ß√µes antes de subir em produ√ß√£o.  
- **Governan√ßa de Dados:** integra√ß√£o com cat√°logos de dados (ex.: **Unity Catalog**, **Purview**) para melhor rastreabilidade, versionamento e gest√£o de acesso.  
- **Qualidade e Valida√ß√£o de Dados:** uso de ferramentas como **Great Expectations** para validar schemas, detectar anomalias e garantir consist√™ncia antes de promover dados para Silver/Gold.  
- **Orquestra√ß√£o Completa:** adotar um orquestrador dedicado (ex.: **Airflow**, **Prefect**, ou **Azure Data Factory**) para controlar tanto ingest√£o quanto processamento, permitindo maior automa√ß√£o e depend√™ncias entre pipelines.  
- **Observabilidade e Alertas:** centralizar m√©tricas, logs e alertas em ferramentas como **Azure Monitor** ou **Grafana**, possibilitando detec√ß√£o proativa de falhas.  
- **Testes Automatizados:** expandir o uso de testes unit√°rios e de integra√ß√£o para notebooks e microservi√ßos, garantindo maior confiabilidade nas mudan√ßas de c√≥digo.  
- **Custo e Performance:** analisar otimiza√ß√µes de custo (storage tiers, autoscaling de clusters no Databricks) e desempenho (particionamento e otimiza√ß√£o de tabelas Delta).  

###  5.2 Considera√ß√µes Finais

O projeto demonstrou como √© poss√≠vel integrar dados de m√∫ltiplas fontes e formatos, organizando-os em um pipeline escal√°vel e confi√°vel na Azure.  
A solu√ß√£o aplicou boas pr√°ticas de engenharia de dados (arquitetura medalh√£o, uso de Delta Lake, pipelines automatizados) em conjunto com boas pr√°ticas de engenharia de software (microservi√ßos, CI/CD, IaC).  

Embora ainda existam pontos de evolu√ß√£o, a arquitetura atual j√° fornece uma base s√≥lida para ingest√£o, transforma√ß√£o e disponibiliza√ß√£o de dados prontos para an√°lise, podendo ser expandida gradualmente conforme as necessidades de neg√≥cio e requisitos de governan√ßa cres√ßam.

##  üìö 6. Refer√™ncias